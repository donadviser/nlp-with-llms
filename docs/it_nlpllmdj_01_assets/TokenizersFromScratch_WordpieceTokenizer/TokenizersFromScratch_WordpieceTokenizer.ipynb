{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KR5LaLggt0f",
        "outputId": "d7e62815-6f8d-4afe-ac59-35d7d6c57e9d"
      },
      "id": "8KR5LaLggt0f",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.16.1 dill-0.3.7 multiprocess-0.70.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load AG News dataset\n",
        "ag_news_dataset = load_dataset(\"ag_news\", split = \"train[:15000]\")\n",
        "\n",
        "ag_news_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NKOgTjP5GKH",
        "outputId": "64f1bfaa-3f08-445f-ce66-2919cbc63a23"
      },
      "id": "3NKOgTjP5GKH",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 15000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_samples = [example['text'] for example in ag_news_dataset]\n",
        "\n",
        "text_samples[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS6GgM7g5MMb",
        "outputId": "57b63182-6d13-489c-f126-3f1f6decfdd3"
      },
      "id": "zS6GgM7g5MMb",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n",
              " 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.',\n",
              " \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.\",\n",
              " 'Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\\\flows from the main pipeline in southern Iraq after\\\\intelligence showed a rebel militia could strike\\\\infrastructure, an oil official said on Saturday.',\n",
              " 'Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doc link -https://huggingface.co/learn/nlp-course/chapter6/8?fw=pt#building-a-wordpiece-tokenizer-from-scratch"
      ],
      "metadata": {
        "id": "0xqAzYztyjcl"
      },
      "id": "0xqAzYztyjcl"
    },
    {
      "cell_type": "markdown",
      "id": "2860a2b3",
      "metadata": {
        "id": "2860a2b3"
      },
      "source": [
        "List out all huggingface datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ac60ed1",
      "metadata": {
        "id": "9ac60ed1"
      },
      "source": [
        "Building a tokenizer with the 🤗 Tokenizers library, we start by instantiating a Tokenizer object with a model, then set its normalizer, pre_tokenizer, post_processor, and decoder attributes to the values we want.\n",
        "\n",
        "More precisely, the library is built around a central Tokenizer class with the building blocks regrouped in submodules:\n",
        "\n",
        "- normalizers contains all the possible types of Normalizer you can use (complete list here).\n",
        "- pre_tokenizers contains all the possible types of PreTokenizer you can use (complete list here).\n",
        "- models contains the various types of Model you can use, like BPE, WordPiece, and Unigram (complete list here).\n",
        "- trainers contains all the different types of Trainer you can use to train your model on a corpus (one per type of model; complete list here).\n",
        "- post_processors contains the various types of PostProcessor you can use (complete list here).\n",
        "- decoders contains the various types of Decoder you can use to decode the outputs of tokenization (complete list here).\n",
        "\n",
        "For this example, we’ll create a Tokenizer with a WordPiece model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "33a97955",
      "metadata": {
        "id": "33a97955",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d92063b2-f627-469e-eecc-347f8ff9e01b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tokenizers.Tokenizer at 0x5994eefbe7a0>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "\n",
        "tokenizer = Tokenizer(models.WordPiece(unk_token = \"[UNK]\"))\n",
        "\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13b1aca1",
      "metadata": {
        "id": "13b1aca1"
      },
      "source": [
        "We have to specify the unk_token so the model knows what to return when it encounters characters it hasn’t seen before. Other arguments we can set here include the vocab of our model (we’re going to train the model, so we don’t need to set this) and max_input_chars_per_word, which specifies a maximum length for each word (words longer than the value passed will be split).\n",
        "\n",
        "The first step of tokenization is normalization, so let’s begin with that. Since BERT is widely used, there is a BertNormalizer with the classic options we can set for BERT: lowercase and strip_accents, which are self-explanatory; clean_text to remove all control characters and replace repeating spaces with a single one; and handle_chinese_chars, which places spaces around Chinese characters. To replicate the bert-base-uncased tokenizer, we can just set this normalizer\n",
        "\n",
        "We’re also using an NFD Unicode normalizer, as otherwise the StripAccents normalizer won’t properly recognize the accented characters and thus won’t strip them out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3bc39c8d",
      "metadata": {
        "id": "3bc39c8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9befbf75-c446-4fb3-cfa2-f6a09d7dfd9c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The pièce de résistance was the chef's Special Dessert.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "tokenizer.normalizer = normalizers.BertNormalizer(lowercase = False)\n",
        "\n",
        "tokenizer.normalizer.normalize_str(\"The pièce de résistance was the chef's Special Dessert.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.normalizer = normalizers.BertNormalizer(lowercase = True)\n",
        "\n",
        "tokenizer.normalizer.normalize_str(\"The pièce de résistance was the chef's Special Dessert.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ujBBYAMuh-KL",
        "outputId": "eee517e1-33e8-4c2f-939f-ec49b4a6797f"
      },
      "id": "ujBBYAMuh-KL",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" the piece de resistance was the chef's special dessert.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "261da405",
      "metadata": {
        "id": "261da405"
      },
      "source": [
        "Generally speaking, however, when building a new tokenizer you won’t have access to such a handy normalizer already implemented in the 🤗 Tokenizers library — so let’s see how to create the BERT normalizer by hand. The library provides a Lowercase normalizer and a StripAccents normalizer, and you can compose several normalizers using a Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "93aee911",
      "metadata": {
        "id": "93aee911",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dc7e05b5-dd88-4961-9fbb-a618a0c06c56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"  the piece de resistance was the chef's special dessert.   \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
        ")\n",
        "\n",
        "tokenizer.normalizer.normalize_str(\"  The pièce de résistance was the chef's Special Dessert.   \")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.normalizer = normalizers.Sequence(\n",
        "    [normalizers.Strip(), normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()]\n",
        ")\n",
        "\n",
        "tokenizer.normalizer.normalize_str(\"   The pièce de résistance was the chef's Special Dessert.   \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "R3WTwi8Gjd6u",
        "outputId": "4c2eb568-f135-4cb4-cc70-53117f75172a"
      },
      "id": "R3WTwi8Gjd6u",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"the piece de resistance was the chef's special dessert.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ab2d66d",
      "metadata": {
        "id": "5ab2d66d"
      },
      "source": [
        "Next is the pre-tokenization step. Again, there is a prebuilt BertPreTokenizer that we can use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "f242aadd",
      "metadata": {
        "id": "f242aadd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0dcb9fb-0dcc-4ecb-b9f8-650f20b74bfd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('We', (0, 2)),\n",
              " (\"'\", (2, 3)),\n",
              " ('re', (3, 5)),\n",
              " ('checking', (6, 14)),\n",
              " ('pre', (15, 18)),\n",
              " ('-', (18, 19)),\n",
              " ('tokenization', (19, 31)),\n",
              " ('step', (32, 36)),\n",
              " ('.', (36, 37))]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
        "\n",
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"We're checking pre-tokenization step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ceb5c14",
      "metadata": {
        "id": "4ceb5c14"
      },
      "source": [
        "Building tokenizer from scratch\n",
        "\n",
        "pre_tokenizers.Whitespace(): Splits on whitespace and punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "30caee8d",
      "metadata": {
        "id": "30caee8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "472c65bf-cdd5-47e8-b04a-bc7ace36f9e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('We', (0, 2)),\n",
              " (\"'\", (2, 3)),\n",
              " ('re', (3, 5)),\n",
              " ('checking', (6, 14)),\n",
              " ('pre', (15, 18)),\n",
              " ('-', (18, 19)),\n",
              " ('tokenization', (19, 31)),\n",
              " ('step', (32, 36)),\n",
              " ('.', (36, 37))]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"We're checking the pre-tokenization step.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b52318d4",
      "metadata": {
        "id": "b52318d4",
        "outputId": "882cbee7-0db2-4f3a-9289-887e5de7a0f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('We', (0, 2)),\n",
              " (\"'\", (2, 3)),\n",
              " ('re', (3, 5)),\n",
              " ('checking', (6, 14)),\n",
              " ('pretokenization', (15, 30)),\n",
              " ('step', (31, 35)),\n",
              " ('.', (35, 36))]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pre_tokenizer.pre_tokenize_str(\"We're checking the pretokenization step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9add85a",
      "metadata": {
        "id": "a9add85a"
      },
      "source": [
        "If we only want to split on whitespace, you should use the WhitespaceSplit pre-tokenizer instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "38c274f2",
      "metadata": {
        "id": "38c274f2",
        "outputId": "0e41f56c-7474-4662-bf40-3eaa5b78e377",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"We're\", (0, 5)),\n",
              " ('checking', (6, 14)),\n",
              " ('pre-tokenization', (15, 31)),\n",
              " ('step.', (32, 37))]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
        "\n",
        "pre_tokenizer.pre_tokenize_str(\"We're checking the pre-tokenization step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5dc1d8a",
      "metadata": {
        "id": "a5dc1d8a"
      },
      "source": [
        "Like with normalizers, you can use a Sequence to compose several pre-tokenizers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "3ec0e6ca",
      "metadata": {
        "id": "3ec0e6ca",
        "outputId": "bf893db1-1115-4b87-83fa-015a55838dfc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('We', (0, 2)),\n",
              " (\"'\", (2, 3)),\n",
              " ('re', (3, 5)),\n",
              " ('checking', (6, 14)),\n",
              " ('the', (15, 18)),\n",
              " ('pre', (19, 22)),\n",
              " ('-', (22, 23)),\n",
              " ('tokenization', (23, 35)),\n",
              " ('step', (36, 40)),\n",
              " ('.', (40, 41))]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "pre_tokenizer = pre_tokenizers.Sequence(\n",
        "    [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()]\n",
        ")\n",
        "\n",
        "pre_tokenizer.pre_tokenize_str(\"We're checking the pre-tokenization step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "299db42a",
      "metadata": {
        "id": "299db42a"
      },
      "source": [
        "The next step in the tokenization pipeline is running the inputs through the model. We already specified our model in the initialization, but we still need to train it, which will require a WordPieceTrainer. The main thing to remember when instantiating a trainer in 🤗 Tokenizers is that you need to pass it all the special tokens you intend to use — otherwise it won’t add them to the vocabulary, since they are not in the training corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c30a4685",
      "metadata": {
        "id": "c30a4685",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61068e13-03a9-4c82-c709-b92a2e3bcf7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tokenizers.trainers.WordPieceTrainer at 0x7cdc9a7619d0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "\n",
        "trainer = trainers.WordPieceTrainer(vocab_size = 20000, special_tokens = special_tokens, min_frequency = 2)\n",
        "\n",
        "trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71598d85",
      "metadata": {
        "id": "71598d85"
      },
      "source": [
        "As well as specifying the vocab_size and special_tokens, we can set the min_frequency (the number of times a token must appear to be included in the vocabulary) or change the continuing_subword_prefix (if we want to use something different from ##).\n",
        "\n",
        "To train our model using the iterator we defined earlier, we just have to execute this command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "961273b1",
      "metadata": {
        "id": "961273b1"
      },
      "outputs": [],
      "source": [
        "tokenizer.train_from_iterator(text_samples, trainer = trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b80820e4",
      "metadata": {
        "id": "b80820e4"
      },
      "source": [
        "Testing our tokenizer by calling encode method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c32e22f1",
      "metadata": {
        "id": "c32e22f1",
        "outputId": "7b9d382e-e7ac-4d20-d3fc-609ddcc3bfab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Encoding(num_tokens=11, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"We're checking pre-tokenization step.\")\n",
        "\n",
        "encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b36ad94",
      "metadata": {
        "id": "7b36ad94"
      },
      "source": [
        "Viewing all the attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "9b800f0f",
      "metadata": {
        "id": "9b800f0f",
        "outputId": "1f77da5f-ac66-4369-968c-5c4c2ef80f27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding_ids : [448, 10, 140, 16701, 644, 15, 131, 11612, 2589, 2548, 16]\n",
            "Encoding_tokens : ['we', \"'\", 're', 'checking', 'pre', '-', 'to', '##ken', '##ization', 'step', '.']\n",
            "Encoding_offsets : [(0, 2), (2, 3), (3, 5), (6, 14), (15, 18), (18, 19), (19, 21), (21, 24), (24, 31), (32, 36), (36, 37)]\n"
          ]
        }
      ],
      "source": [
        "print(\"Encoding_ids :\", encoding.ids)\n",
        "\n",
        "print(\"Encoding_tokens :\", encoding.tokens)\n",
        "\n",
        "print(\"Encoding_offsets :\", encoding.offsets)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d7604e8",
      "metadata": {
        "id": "7d7604e8"
      },
      "source": [
        "The encoding obtained is an Encoding, which contains all the necessary outputs of the tokenizer in its various attributes: ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, and overflowing.\n",
        "\n",
        "The last step in the tokenization pipeline is post-processing. We need to add the [CLS] token at the beginning and the [SEP] token at the end (or after each sentence, if we have a pair of sentences). We will use a TemplateProcessor for this, but first we need to know the IDs of the [CLS] and [SEP] tokens in the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "180d4ad4",
      "metadata": {
        "id": "180d4ad4",
        "outputId": "09bc807c-f35e-4424-86cd-5a1dd767e291",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 3\n"
          ]
        }
      ],
      "source": [
        "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
        "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
        "\n",
        "print(cls_token_id, sep_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df406aba",
      "metadata": {
        "id": "df406aba"
      },
      "source": [
        "To write the template for the TemplateProcessor, we have to specify how to treat a single sentence and a pair of sentences. For both, we write the special tokens we want to use; the first (or single) sentence is represented by `$A`, while the second sentence (if encoding a pair) is represented by `$B`. For each of these (special tokens and sentences), we also specify the corresponding token type ID after a colon.\n",
        "\n",
        "The classic BERT template is thus defined as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "15055c10",
      "metadata": {
        "id": "15055c10"
      },
      "outputs": [],
      "source": [
        "tokenizer.post_processor = processors.TemplateProcessing(\n",
        "    single = f\"[CLS]:0 $A:0 [SEP]:0\",\n",
        "    pair = f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
        "    special_tokens = [(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61df5ab6",
      "metadata": {
        "id": "61df5ab6"
      },
      "source": [
        "Note that we need to pass along the IDs of the special tokens, so the tokenizer can properly convert them to their IDs. Once this is added, going back to our previous example will give"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "d25c5aed",
      "metadata": {
        "id": "d25c5aed",
        "outputId": "136e4b06-6d55-4900-bd74-8158ab37b86b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding_ids : [2, 448, 10, 140, 16701, 644, 15, 131, 11612, 2589, 2548, 16, 3]\n",
            "\n",
            "Encoding_tokens : ['[CLS]', 'we', \"'\", 're', 'checking', 'pre', '-', 'to', '##ken', '##ization', 'step', '.', '[SEP]']\n",
            "\n",
            "Encoding_offsets : [(0, 0), (0, 2), (2, 3), (3, 5), (6, 14), (15, 18), (18, 19), (19, 21), (21, 24), (24, 31), (32, 36), (36, 37), (0, 0)]\n",
            "\n",
            "Encoding_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"We're checking pre-tokenization step.\")\n",
        "\n",
        "print(\"Encoding_ids :\", encoding.ids)\n",
        "print()\n",
        "print(\"Encoding_tokens :\", encoding.tokens)\n",
        "print()\n",
        "print(\"Encoding_offsets :\", encoding.offsets)\n",
        "print()\n",
        "print(\"Encoding_type_ids :\", encoding.type_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ffd9185",
      "metadata": {
        "id": "1ffd9185"
      },
      "source": [
        "And on a pair of sentences, we get the proper result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "b91f5169",
      "metadata": {
        "id": "b91f5169",
        "outputId": "b1b0589a-b015-415c-a2a8-55c0e1929bd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding_ids : [2, 448, 10, 140, 16701, 644, 15, 131, 11612, 2589, 2548, 16, 3, 150, 34, 3189, 134, 12765, 16, 3]\n",
            "\n",
            "Encoding_tokens : ['[CLS]', 'we', \"'\", 're', 'checking', 'pre', '-', 'to', '##ken', '##ization', 'step', '.', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']\n",
            "\n",
            "Encoding_offsets : [(0, 0), (0, 2), (2, 3), (3, 5), (6, 14), (15, 18), (18, 19), (19, 21), (21, 24), (24, 31), (32, 36), (36, 37), (0, 0), (0, 2), (3, 4), (5, 9), (10, 12), (13, 22), (22, 23), (0, 0)]\n",
            "\n",
            "Encoding_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ],
      "source": [
        "encoding = tokenizer.encode(\"We're checking pre-tokenization step.\", \"on a pair of sentences.\")\n",
        "\n",
        "print(\"Encoding_ids :\", encoding.ids)\n",
        "print()\n",
        "print(\"Encoding_tokens :\", encoding.tokens)\n",
        "print()\n",
        "print(\"Encoding_offsets :\", encoding.offsets)\n",
        "print()\n",
        "print(\"Encoding_type_ids :\", encoding.type_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caed36f3",
      "metadata": {
        "id": "caed36f3"
      },
      "source": [
        "We’ve almost finished building this tokenizer from scratch — the last step is to include a decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "95fb3afb",
      "metadata": {
        "id": "95fb3afb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fcfdb3cc-f7ee-4c91-b9fc-a93cbde353d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"we ' re checking pre - tokenization step.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "tokenizer.decoder = decoders.WordPiece(prefix = \"##\")\n",
        "\n",
        "tokenizer.decode(encoding.ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "085e0f00",
      "metadata": {
        "id": "085e0f00"
      },
      "outputs": [],
      "source": [
        "tokenizer.save(\"word_piece_tokenizer.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ab05d44",
      "metadata": {
        "id": "0ab05d44"
      },
      "source": [
        "We can then reload that file in a Tokenizer object with the from_file() method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "a85e0565",
      "metadata": {
        "id": "a85e0565",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5e00ca9-6ffd-4945-d366-0cf7135ff1b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tokenizers.Tokenizer at 0x5994eee29ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "new_tokenizer = Tokenizer.from_file(\"word_piece_tokenizer.json\")\n",
        "\n",
        "new_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = new_tokenizer.encode(\"We're checking pre-tokenization step.\", \"on a pair of sentences.\")\n",
        "\n",
        "print(\"Encoding_ids :\", encoding.ids)\n",
        "print()\n",
        "print(\"Encoding_tokens :\", encoding.tokens)\n",
        "print()\n",
        "print(\"Encoding_offsets :\", encoding.offsets)\n",
        "print()\n",
        "print(\"Encoding_type_ids :\", encoding.type_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lyrf804UlBDK",
        "outputId": "f0f76e1c-0719-4ff9-fb8c-38a4e0a8d137"
      },
      "id": "Lyrf804UlBDK",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding_ids : [2, 448, 10, 140, 16701, 644, 15, 131, 11612, 2589, 2548, 16, 3, 150, 34, 3189, 134, 12765, 16, 3]\n",
            "\n",
            "Encoding_tokens : ['[CLS]', 'we', \"'\", 're', 'checking', 'pre', '-', 'to', '##ken', '##ization', 'step', '.', '[SEP]', 'on', 'a', 'pair', 'of', 'sentences', '.', '[SEP]']\n",
            "\n",
            "Encoding_offsets : [(0, 0), (0, 2), (2, 3), (3, 5), (6, 14), (15, 18), (18, 19), (19, 21), (21, 24), (24, 31), (32, 36), (36, 37), (0, 0), (0, 2), (3, 4), (5, 9), (10, 12), (13, 22), (22, 23), (0, 0)]\n",
            "\n",
            "Encoding_type_ids : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a654117b",
      "metadata": {
        "id": "a654117b"
      },
      "source": [
        "To use this tokenizer in 🤗 Transformers, we have to wrap it in a PreTrainedTokenizerFast. We can either use the generic class or, if our tokenizer corresponds to an existing model, use that class (here, BertTokenizerFast). If you apply this lesson to build a brand new tokenizer, you will have to use the first option.\n",
        "\n",
        "To wrap the tokenizer in a PreTrainedTokenizerFast, we can either pass the tokenizer we built as a tokenizer_object or pass the tokenizer file we saved as tokenizer_file. The key thing to remember is that we have to manually set all the special tokens, since that class can’t infer from the tokenizer object which token is the mask token, the [CLS] token, etc.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "ec8673d8",
      "metadata": {
        "id": "ec8673d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37aaa188-72f9-49f9-f468-93c8de7c1574"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['we',\n",
              " \"'\",\n",
              " 're',\n",
              " 'checking',\n",
              " 'pre',\n",
              " '-',\n",
              " 'to',\n",
              " '##ken',\n",
              " '##ization',\n",
              " 'step',\n",
              " '.',\n",
              " 'on',\n",
              " 'a',\n",
              " 'pair',\n",
              " 'of',\n",
              " 'sentences',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object = tokenizer,\n",
        "    unk_token = \"[UNK]\",\n",
        "    pad_token = \"[PAD]\",\n",
        "    cls_token = \"[CLS]\",\n",
        "    sep_token = \"[SEP]\",\n",
        "    mask_token = \"[MASK]\",\n",
        ")\n",
        "\n",
        "wrapped_tokenizer.tokenize(\"We're checking pre-tokenization step.\", \"on a pair of sentences.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c33ae7f3",
      "metadata": {
        "id": "c33ae7f3"
      },
      "source": [
        "If we are using a specific tokenizer class (like BertTokenizerFast), we  only need to specify the special tokens that are different from the default ones (here, none):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "72ee78ab",
      "metadata": {
        "id": "72ee78ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecc1dc2b-41c0-4519-f88c-143e5083c000"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['we',\n",
              " \"'\",\n",
              " 're',\n",
              " 'checking',\n",
              " 'pre',\n",
              " '-',\n",
              " 'to',\n",
              " '##ken',\n",
              " '##ization',\n",
              " 'step',\n",
              " '.',\n",
              " 'on',\n",
              " 'a',\n",
              " 'pair',\n",
              " 'of',\n",
              " 'sentences',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)\n",
        "\n",
        "wrapped_tokenizer.tokenize(\"We're checking pre-tokenization step.\", \"on a pair of sentences.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "c1c678df",
      "metadata": {
        "id": "c1c678df"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "keras_venv",
      "language": "python",
      "name": "keras_venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}