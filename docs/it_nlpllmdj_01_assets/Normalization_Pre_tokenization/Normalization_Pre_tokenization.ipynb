{"cells":[{"cell_type":"markdown","source":["https://huggingface.co/learn/nlp-course/chapter6/4#normalization-and-pre-tokenization"],"metadata":{"id":"WAVdxvDZz3Z2"},"id":"WAVdxvDZz3Z2"},{"cell_type":"markdown","id":"c06f9631","metadata":{"id":"c06f9631"},"source":["Normalization is, in a nutshell, a set of operations you apply to a raw string to make it less random or ‚Äúcleaner‚Äù. Common operations include stripping whitespace, removing accented characters or lowercasing all text. If you‚Äôre familiar with Unicode normalization, it is also a very common normalization operation applied in most tokenizers.\n","\n","Each normalization operation is represented in the ü§ó Tokenizers library by a Normalizer, and you can combine several of those by using a normalizers.Sequence. Here is a normalizer applying NFD Unicode normalization and removing accents as an example:"]},{"cell_type":"markdown","source":["### TODO Recording:\n","\n","- Go to https://colab.research.google.com/\n","- Login with your account, create a new notebook and give the name of this notebook\n","- Show that you are using the regular CPU runtime\n","- Setting up a hugging face secret token to access HF from Colab (this is optional right now but will become compulsory later)\n","- Go to https://huggingface.co/ (you should already be logged in)\n","- Click on the account icon at the top-right -> go to Settings\n","- Click on Access Tokens from the left\n","- Create a new token with WRITE privileges\n","- Copy the token over and come to this notebook\n","- Click on the key on the left of the screen\n","- Add Secret_Name = `HF_TOKEN`\n","- Add Value = `<token generated>`\n","- Enable notebook access\n","- Now you can write code"],"metadata":{"id":"kUv0C16DdnKs"},"id":"kUv0C16DdnKs"},{"cell_type":"code","execution_count":null,"id":"cba876bf","metadata":{"id":"cba876bf"},"outputs":[],"source":["from tokenizers import normalizers\n","\n","from tokenizers.normalizers import NFD, StripAccents, Lowercase"]},{"cell_type":"markdown","id":"77d3413e","metadata":{"id":"77d3413e"},"source":["We can manually test that normalizer by applying it to any string"]},{"cell_type":"code","source":["normalizer = normalizers.Sequence([Lowercase()])\n","\n","normalizer.normalize_str(\"Caf√© culture is prominent in many cities around the world.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Ry2t4oPDaZ1t","outputId":"03db6323-cf4d-4cf4-92a8-67503a1fbf01"},"id":"Ry2t4oPDaZ1t","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'caf√© culture is prominent in many cities around the world.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","execution_count":null,"id":"64082a8e","metadata":{"id":"64082a8e","outputId":"ef7c03ce-1ead-4a7d-8cff-20c5ccc0d111","colab":{"base_uri":"https://localhost:8080/","height":35}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cafe culture is prominent in many cities around the world.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":21}],"source":["normalizer = normalizers.Sequence([NFD(), StripAccents(), Lowercase()])\n","\n","normalizer.normalize_str(\"Caf√© culture is prominent in many cities around the world.\")"]},{"cell_type":"code","execution_count":null,"id":"fd2dbab6","metadata":{"id":"fd2dbab6","outputId":"6a0b41fa-d086-4657-f444-1dd4e23854cb","colab":{"base_uri":"https://localhost:8080/","height":35}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'the protagonist had deja vu when he entered the old mansion.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}],"source":["normalizer.normalize_str(\"The protagonist had d√©j√† vu when he entered the old mansion.\")"]},{"cell_type":"code","source":["normalizer.normalize_str(\"H√©ll√≤ h√¥w are √º?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"GNzuQE-qhMoH","outputId":"d4393566-0869-49b6-f695-f0ddc4e72676"},"id":"GNzuQE-qhMoH","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'hello how are u?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"code","execution_count":null,"id":"51976415","metadata":{"id":"51976415","outputId":"1e6b721e-d4c6-447f-d72e-cffc554ba259","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('She', (0, 3)),\n"," ('can', (4, 7)),\n"," (\"'\", (7, 8)),\n"," ('t', (8, 9)),\n"," ('attend', (10, 16)),\n"," ('the', (17, 20)),\n"," ('meeting', (21, 28)),\n"," ('due', (29, 32)),\n"," ('to', (33, 35)),\n"," ('prior', (36, 41)),\n"," ('commitments', (42, 53)),\n"," ('.', (53, 54))]"]},"metadata":{},"execution_count":24}],"source":["from tokenizers.pre_tokenizers import Whitespace\n","\n","pre_tokenizer = Whitespace()\n","\n","pre_tokenizer.pre_tokenize_str(\"She can't attend the meeting due to prior commitments.\")"]},{"cell_type":"markdown","id":"fd2c7a84","metadata":{"id":"fd2c7a84"},"source":["We can combine together any PreTokenizer together. For instance, here is a pre-tokenizer that will split on space, punctuation and digits, separating numbers in their individual digits:\n","\n","### TODO Recording:\n","\n","- First run the code with False, then change False -> True and run the code with True"]},{"cell_type":"code","execution_count":null,"id":"2893b081","metadata":{"id":"2893b081","outputId":"d8e623fd-7a9d-4861-86ae-5dbca2580d1e","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('I', (0, 1)),\n"," ('am', (2, 4)),\n"," ('calling', (5, 12)),\n"," ('you', (13, 16)),\n"," ('on', (17, 19)),\n"," ('93457654', (20, 28))]"]},"metadata":{},"execution_count":25}],"source":["from tokenizers import pre_tokenizers\n","from tokenizers.pre_tokenizers import Digits\n","\n","pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits = False)])\n","\n","pre_tokenizer.pre_tokenize_str(\"I am calling you on 93457654\")"]},{"cell_type":"markdown","source":["# Tokenizers used by different transformer models"],"metadata":{"id":"u2E3HwMJbG-I"},"id":"u2E3HwMJbG-I"},{"cell_type":"markdown","source":["### TODO Recording\n","\n","- Go to https://huggingface.co/\n","- Search for bert-base-uncased, show the model card\n","- Come back here to the code\n","\n","\n","\"##\" means that the rest of the token should be attached to the previous one, without space (for decoding or reversal of the tokenization)."],"metadata":{"id":"f0nPlObBbt98"},"id":"f0nPlObBbt98"},{"cell_type":"code","execution_count":null,"id":"42145b41","metadata":{"id":"42145b41","outputId":"6490ad61-08da-4004-c300-c8196aaf831a","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['i', 'have', 'a', 'new', 'samsung', 'g', '##lite']"]},"metadata":{},"execution_count":30}],"source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","tokenizer.tokenize(\"I have a new SAMSUNG GLITE\")"]},{"cell_type":"code","source":["tokenizer.tokenize(\"Hello, y'all! How   are you üòÅ ?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pVHvdAf4hpXy","outputId":"c3805346-000b-42fe-aa9f-15dbb8b9a483"},"id":"pVHvdAf4hpXy","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hello', ',', 'y', \"'\", 'all', '!', 'how', 'are', 'you', '[UNK]', '?']"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["The pre-tokenizer for BERT can be accessed using the backend_tokenizer\n","\n","Notice how the tokenizer is already keeping track of the offsets, which is how it can give us the offset mapping we used in the previous section. Here the tokenizer ignores the two spaces and replaces them with just one, but the offset jumps between are and you to account for that.\n","\n","Since we‚Äôre using a BERT tokenizer, the pre-tokenization involves splitting on whitespace and punctuation."],"metadata":{"id":"yVyoNkgXiPR8"},"id":"yVyoNkgXiPR8"},{"cell_type":"code","source":["tokenizer.backend_tokenizer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uqKC5IAfh_tE","outputId":"6f612350-ed0a-4f07-8d93-655dee5a1fb5"},"id":"uqKC5IAfh_tE","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tokenizers.Tokenizer at 0x57619b550a10>"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"I have a new SAMSUNG GLITE\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4tLgX7UYiXqa","outputId":"948c5745-b913-44ae-f62e-b51635e63f63"},"id":"4tLgX7UYiXqa","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('I', (0, 1)),\n"," ('have', (2, 6)),\n"," ('a', (7, 8)),\n"," ('new', (9, 12)),\n"," ('SAMSUNG', (13, 20)),\n"," ('GLITE', (21, 26))]"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, y'all! How   are you üòÅ ?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yMORzpiXih5Q","outputId":"c33ff2e4-4b48-486e-92e9-6d3496f1adf3"},"id":"yMORzpiXih5Q","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Hello', (0, 5)),\n"," (',', (5, 6)),\n"," ('y', (7, 8)),\n"," (\"'\", (8, 9)),\n"," ('all', (9, 12)),\n"," ('!', (12, 13)),\n"," ('How', (14, 17)),\n"," ('are', (20, 23)),\n"," ('you', (24, 27)),\n"," ('üòÅ', (28, 29)),\n"," ('?', (30, 31))]"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["tokenizer.backend_tokenizer.normalizer.normalize_str(\"The protagonist had d√©j√† vu when he entered the old mansion.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"vPjFwBUnjTTN","outputId":"e65e69bd-f361-4210-da11-d30ead560c2f"},"id":"vPjFwBUnjTTN","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'the protagonist had deja vu when he entered the old mansion.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["tokenizer.backend_tokenizer.normalizer.normalize_str(\"H√©ll√≤ h√¥w are √º?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"VL0YZMA_jQmF","outputId":"6a473c74-b6ae-43af-e31c-6d11c9df4c08"},"id":"VL0YZMA_jQmF","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'hello how are u?'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":37}]},{"cell_type":"markdown","source":["Let's use the GPT-2 tokenizer\n","\n","It will split on whitespace and punctuation as well, but it will keep the spaces and replace them with a ƒ† symbol, enabling it to recover the original spaces if we decode the tokens.\n","\n","Also note that unlike the BERT tokenizer, this tokenizer does not ignore the double space."],"metadata":{"id":"nEGe0kBFjJMA"},"id":"nEGe0kBFjJMA"},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","\n","tokenizer.tokenize(\"I have a new SAMSUNG GLITE\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UtLn87dXjex1","outputId":"7945ff48-5e66-48c5-8804-9304d53993d7"},"id":"UtLn87dXjex1","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['I', 'ƒ†have', 'ƒ†a', 'ƒ†new', 'ƒ†SAM', 'S', 'UN', 'G', 'ƒ†GL', 'ITE']"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["tokenizer.tokenize(\"Hello, y'all! How   are you üòÅ ?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L5walzLgkl5I","outputId":"def2fd55-628a-4bbd-c3c9-5d6366e75522"},"id":"L5walzLgkl5I","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello',\n"," ',',\n"," 'ƒ†y',\n"," \"'\",\n"," 'all',\n"," '!',\n"," 'ƒ†How',\n"," 'ƒ†',\n"," 'ƒ†',\n"," 'ƒ†are',\n"," 'ƒ†you',\n"," 'ƒ†√∞≈Åƒ∫',\n"," 'ƒ£',\n"," 'ƒ†?']"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"I have a new SAMSUNG GLITE\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p0wzM0SDjelW","outputId":"cb0ba06a-9892-40fd-8132-2d01149c2de9"},"id":"p0wzM0SDjelW","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('I', (0, 1)),\n"," ('ƒ†have', (1, 6)),\n"," ('ƒ†a', (6, 8)),\n"," ('ƒ†new', (8, 12)),\n"," ('ƒ†SAMSUNG', (12, 20)),\n"," ('ƒ†GLITE', (20, 26))]"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, y'all! How   are you üòÅ ?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7lbgLMbXkLzF","outputId":"30194ccc-27dd-4d65-9cb5-2e9e527b7b6a"},"id":"7lbgLMbXkLzF","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Hello', (0, 5)),\n"," (',', (5, 6)),\n"," ('ƒ†y', (6, 8)),\n"," (\"'\", (8, 9)),\n"," ('all', (9, 12)),\n"," ('!', (12, 13)),\n"," ('ƒ†How', (13, 17)),\n"," ('ƒ†ƒ†', (17, 19)),\n"," ('ƒ†are', (19, 23)),\n"," ('ƒ†you', (23, 27)),\n"," ('ƒ†√∞≈Åƒ∫ƒ£', (27, 29)),\n"," ('ƒ†?', (29, 31))]"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","source":["Let's use the T5-small tokenizer (this uses the SentencePiece algorithm)\n","\n","Like the GPT-2 tokenizer, this one keeps spaces and replaces them with a specific token (_), but the T5 tokenizer only splits on whitespace, not punctuation. Also note that it added a space by default at the beginning of the sentence and ignored the double space.\n","\n"],"metadata":{"id":"DV-FHwP0k1D6"},"id":"DV-FHwP0k1D6"},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n","\n","tokenizer.tokenize(\"I have a new SAMSUNG GLITE\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BA37IOaokiI8","outputId":"faa013fc-98e9-4ef0-dbd6-fd2a2c0256b5"},"id":"BA37IOaokiI8","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['‚ñÅI', '‚ñÅhave', '‚ñÅ', 'a', '‚ñÅnew', '‚ñÅS', 'AMS', 'UNG', '‚ñÅ', 'GL', 'ITE']"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["tokenizer.tokenize(\"Hello, y'all! How   are you üòÅ ?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ea_uFNk4kiBu","outputId":"3c65a3b1-7bc4-4b5c-80e3-8778cf28983a"},"id":"ea_uFNk4kiBu","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['‚ñÅHello',\n"," ',',\n"," '‚ñÅ',\n"," 'y',\n"," \"'\",\n"," 'all',\n"," '!',\n"," '‚ñÅHow',\n"," '‚ñÅare',\n"," '‚ñÅyou',\n"," '‚ñÅ',\n"," 'üòÅ',\n"," '‚ñÅ',\n"," '?']"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"I have a new SAMSUNG GLITE\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LrR0xxMtkh7d","outputId":"eed68e97-efd3-4bcd-e01d-3a1e73724e10"},"id":"LrR0xxMtkh7d","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('‚ñÅI', (0, 1)),\n"," ('‚ñÅhave', (2, 6)),\n"," ('‚ñÅa', (7, 8)),\n"," ('‚ñÅnew', (9, 12)),\n"," ('‚ñÅSAMSUNG', (13, 20)),\n"," ('‚ñÅGLITE', (21, 26))]"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(\"Hello, y'all! How   are you üòÅ ?\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MHdNNIZwkh52","outputId":"ec253713-98ff-4faf-d03c-36b9a8cf0c55"},"id":"MHdNNIZwkh52","execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('‚ñÅHello,', (0, 6)),\n"," (\"‚ñÅy'all!\", (7, 13)),\n"," ('‚ñÅHow', (14, 17)),\n"," ('‚ñÅare', (20, 23)),\n"," ('‚ñÅyou', (24, 27)),\n"," ('‚ñÅüòÅ', (28, 29)),\n"," ('‚ñÅ?', (30, 31))]"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","execution_count":null,"id":"d910c82c","metadata":{"id":"d910c82c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"52c877b3","metadata":{"id":"52c877b3"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"4a8d7a08","metadata":{"id":"4a8d7a08"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"33a97955","metadata":{"id":"33a97955"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"3bc39c8d","metadata":{"id":"3bc39c8d"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"93aee911","metadata":{"id":"93aee911"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"c3bfc8dd","metadata":{"id":"c3bfc8dd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"f242aadd","metadata":{"id":"f242aadd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"30caee8d","metadata":{"id":"30caee8d"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"b52318d4","metadata":{"id":"b52318d4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"38c274f2","metadata":{"id":"38c274f2"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"3ec0e6ca","metadata":{"id":"3ec0e6ca"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"c30a4685","metadata":{"id":"c30a4685"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"keras_venv","language":"python","name":"keras_venv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}