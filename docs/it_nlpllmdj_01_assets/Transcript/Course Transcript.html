<!DOCTYPE html>
<!-- saved from url=(0164)https://cdn2.percipio.com/secure/c/1739516003.dfd1fe81ec26d85052b7349f7f75bee477424294/eot/transcripts/b3ce0952-f71e-4974-a009-84daef8b6b56/it_nlpllmdj_01_enus.html -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="content-style-type" content="text/css2"><title>Course Transcript</title><link href="./Course Transcript_files/css" rel="stylesheet" type="text/css"><style>:root {
    --title-text: #243038;
    --toc-link: #0073C4;
    --section-title-text: #243038;
    --section-title-border: #bababa;
    --grid-caption-text: #164794;
    --grid-border: #c0c0c0;
    --grid-hdr-text: #973131;
    --grid-hdr-background: #f7f7f7;
    --missing-data: #767676;
    --text-color: #343434;
    --lightbox-border: #a0a0a0;

    --bold-font-weight: 700;

    font-family: Lato;
}

.report {
    margin: 24px;
}

.container {
    position: relative;
}

#lb-curtain {
    background-color: white;
    opacity: 1.0;
}

#lb-overlay {
    position: absolute;
    top: 40px;
    left: 20px;
    right: 20px;
    background-color: white;
    border: solid 1px var(--lightbox-border);
    visibility: hidden;
}

.lb-header, .lb-footer {
    height: 40px;
    line-height: 40px;
}

.lb-header {
    display: flex;
    justify-content: flex-end;
    padding-right: 20px;
}

#lb-image-container {
    margin: 0 40px;
}

#lb-image {
    width: 100%;
}

.report_title {
    font-size: 22px;
    font-weight: var(--bold-font-weight);
    color: var(--title-text);
}

.toc_link {
    font-size: 14px;
    color: var(--toc-link);
    margin-top: 6px;
    margin-bottom: 0;
}

.separator {
    margin: 12px 0 0;
}

.section {
    margin-top: 40px;
    margin-left: 16px;
}

.section.inner {
    margin-top: 20px;
}

.section.inline {
    margin-top: 20px;
    margin-left: 0;
}

.section_title {
    font-size: 18px;
    font-weight: var(--bold-font-weight);
    width: 100%;
    color: var(--section-title-text);
}

h2.section_title {
    border-bottom-color: var(--section-title-border);
    border-bottom-style: solid;
    border-bottom-width: thin;
}

h3.section_title {
    margin: 8px 0;
}

h3.list_title {
    margin: 8px 0;
    font-size: 14px;
    color: var(--title-text);
}

.section_text {
    font-size: 14px;
    color: var(--text-color);
    margin: 12px 0;
    width: 90%;
}

.section_link {
    font-size: 14px;
    margin: 12px 0;
    width: 90%;
}

.grid {
    border-collapse: collapse;
}
.grid_caption {
    font-size: 14px;
    font-weight: var(--bold-font-weight);
    color: var(--grid-caption-text);
    margin: 12px 0;
    text-align: left;
    white-space: nowrap;
}
.grid_hdr_row {

}
.grid_col {
    font-size: 13px;
    color: var(--text-color);
    border-collapse: collapse;
    padding: 2px;
    text-align: left;
    vertical-align: top;
}
.grid_col_hdr, .grid_row_hdr {
    font-size: 12px;
    font-weight: var(--bold-font-weight);
    white-space: nowrap;
}
.grid_row_hdr {

}

.empty_grid {
    border-collapse: collapse;
    border-color: var(--grid-border);
    border-style: solid;
    border-width: 2px;
    padding: 4px;
    text-align: center;
}

.htmlbox {
    border: solid 1px var(--grid-border);
    box-shadow: 1px 1px 4px var(--grid-border);
    padding: 8px;
}

.list {
    font-size: 14px;
    font-style: italic;
}

.missing_data {
    color: var(--missing-data);
    font-size: 14px;
    font-weight: var(--bold-font-weight);
    font-style: italic;
}

.copyright-container {
    margin-top: 24px;
    text-align: center;
}
.copyright-text {
    color: var(--text-color);
    font-size: 12px;
}

/* Transcript Questions CSS */

.question {
    border: solid 1px var(--grid-border);
    box-shadow: 1px 1px 4px var(--grid-border);
    display: flex;
    flex-direction: column;
    padding: 8px;
}

.question_content {
    display: flex;
    justify-content: space-between;
}

.question_details {
    display: flex;
    flex-direction: column;
    width: 100%;
}

.question_images {
    display: flex;
    flex-direction: column;
    margin-left: 16px;
}

.question_image {
    margin-bottom: 8px;
}

.question_title {
    font-weight: var(--bold-font-weight);
    border-bottom-color: var(--grid-border);
    border-bottom-style: solid;
    border-bottom-width: 1px;
    padding-bottom: 8px;
    margin-bottom: 8px;
}

.question_label, .question_stem {
    font-weight: var(--bold-font-weight);
    margin-top: 8px;
    margin-bottom: 8px;
}

.question_data_row {
    display: flex;
}

.question_data_col1 {
    padding: 8px;
    white-space: nowrap;
}

.question_data_col2 {
    padding: 8px;
    display: flex;
    flex-direction: column;
}

.question_answer_key {
    margin-top: 8px;
    border-bottom-color: var(--grid-border);
    border-bottom-style: solid;
    border-bottom-width: 1px;
}

.feedback {
    font-style: italic;
}
</style></head><body class="report"><div class="container"><div class="content" id="lb-curtain"><h1 class="report_title">NLP with LLMs: Working with Tokenizers in Hugging Face</h1><div class="section_text">Hugging Face, a leading company in the field of artificial intelligence (AI), offers a comprehensive platform that enables developers and researchers to build, train, and deploy state-of-the-art machine learning (ML) models with a strong emphasis on open collaboration and community-driven development.

In this course, you will discover the extensive libraries and tools Hugging Face offers, including the Transformers library, which provides access to a vast array of pre-trained models and datasets. 

Next, you will set up your working environment in Google Colab. You will also explore the critical components of the text preprocessing pipeline: normalizers and pre-tokenizers.

Finally, you will master various tokenization techniques, including byte pair encoding (BPE), Wordpiece, and Unigram tokenization, which are essential for working with transformer models. Through hands-on exercises, you will build and train BPE and WordPiece tokenizers, configuring normalizers and pre-tokenizers to fine-tune these tokenization methods. </div><div class="section"><h2 class="section_title">Table of Contents</h2><div class="toc"><p class="toc_link">&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://cdn2.percipio.com/secure/c/1739516003.dfd1fe81ec26d85052b7349f7f75bee477424294/eot/transcripts/b3ce0952-f71e-4974-a009-84daef8b6b56/it_nlpllmdj_01_enus.html#section_0">1. Video: Course Overview (it_nlpllmdj_01_enus_01)</a></p><p class="toc_link">&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://cdn2.percipio.com/secure/c/1739516003.dfd1fe81ec26d85052b7349f7f75bee477424294/eot/transcripts/b3ce0952-f71e-4974-a009-84daef8b6b56/it_nlpllmdj_01_enus.html#section_1">2. Video: Hugging Face Introduction (it_nlpllmdj_01_enus_02)</a></p><p class="toc_link">&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://cdn2.percipio.com/secure/c/1739516003.dfd1fe81ec26d85052b7349f7f75bee477424294/eot/transcripts/b3ce0952-f71e-4974-a009-84daef8b6b56/it_nlpllmdj_01_enus.html#section_2">3. Video: Hugging Face Tokenizers (it_nlpllmdj_01_enus_03)</a></p><p class="toc_link">&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://cdn2.percipio.com/secure/c/1739516003.dfd1fe81ec26d85052b7349f7f75bee477424294/eot/transcripts/b3ce0952-f71e-4974-a009-84daef8b6b56/it_nlpllmdj_01_enus.html#section_3">4. Video: Exploring the Hugging Face Platform (it_nlpllmdj_01_enus_04)</a></p><p class="toc_link">&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://cdn2.percipio.com/secure/c/1739516003.dfd1fe81ec26d85052b7349f7f75bee477424294/eot/transcripts/b3ce0952-f71e-4974-a009-84daef8b6b56/it_nlpllmdj_01_enus.html#section_4">5. Video: Setting up the Colab Environment (it_nlpllmdj_01_enus_05)</a></p><p class="toc_link">&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://cdn2.percipio.com/secure/c/1739516003.dfd1fe81ec26d85052b7349f7f75bee477424294/eot/transcripts/b3ce0952-f71e-4974-a009-84daef8b6b56/it_nlpllmdj_01_enus.html#section_5">6. Video: Normalizers and Pre-tokenizers (it_nlpllmdj_01_enus_06)</a></p><p class="toc_link">&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://cdn2.percipio.com/secure/c/1739516003.dfd1fe81ec26d85052b7349f7f75bee477424294/eot/transcripts/b3ce0952-f71e-4974-a009-84daef8b6b56/it_nlpllmdj_01_enus.html#section_6">7. Video: Byte Pair Encoding (BPE), Wordpiece, and Unigram Tokenization (it_nlpllmdj_01_enus_07)</a></p><p class="toc_link">&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://cdn2.percipio.com/secure/c/1739516003.dfd1fe81ec26d85052b7349f7f75bee477424294/eot/transcripts/b3ce0952-f71e-4974-a009-84daef8b6b56/it_nlpllmdj_01_enus.html#section_7">8. Video: Implementing Byte Pair Encoding Tokenization - I (it_nlpllmdj_01_enus_08)</a></p><p class="toc_link">&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://cdn2.percipio.com/secure/c/1739516003.dfd1fe81ec26d85052b7349f7f75bee477424294/eot/transcripts/b3ce0952-f71e-4974-a009-84daef8b6b56/it_nlpllmdj_01_enus.html#section_8">9. Video: Implementing Byte Pair Encoding Tokenization - II (it_nlpllmdj_01_enus_09)</a></p><p class="toc_link">&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://cdn2.percipio.com/secure/c/1739516003.dfd1fe81ec26d85052b7349f7f75bee477424294/eot/transcripts/b3ce0952-f71e-4974-a009-84daef8b6b56/it_nlpllmdj_01_enus.html#section_9">10. Video: Implementing Wordpiece Tokenization - I (it_nlpllmdj_01_enus_10)</a></p><p class="toc_link">&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://cdn2.percipio.com/secure/c/1739516003.dfd1fe81ec26d85052b7349f7f75bee477424294/eot/transcripts/b3ce0952-f71e-4974-a009-84daef8b6b56/it_nlpllmdj_01_enus.html#section_10">11. Video: Implementing Wordpiece Tokenization - II (it_nlpllmdj_01_enus_11)</a></p><p class="toc_link">&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://cdn2.percipio.com/secure/c/1739516003.dfd1fe81ec26d85052b7349f7f75bee477424294/eot/transcripts/b3ce0952-f71e-4974-a009-84daef8b6b56/it_nlpllmdj_01_enus.html#section_11">12. Video: Building and Training a BPE Tokenizer (it_nlpllmdj_01_enus_12)</a></p><p class="toc_link">&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://cdn2.percipio.com/secure/c/1739516003.dfd1fe81ec26d85052b7349f7f75bee477424294/eot/transcripts/b3ce0952-f71e-4974-a009-84daef8b6b56/it_nlpllmdj_01_enus.html#section_12">13. Video: Configuring the Normalizer and Pre-tokenizer for Wordpiece Tokenization (it_nlpllmdj_01_enus_13)</a></p><p class="toc_link">&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://cdn2.percipio.com/secure/c/1739516003.dfd1fe81ec26d85052b7349f7f75bee477424294/eot/transcripts/b3ce0952-f71e-4974-a009-84daef8b6b56/it_nlpllmdj_01_enus.html#section_13">14. Video: Building and Training a Wordpiece Tokenizer (it_nlpllmdj_01_enus_14)</a></p><p class="toc_link">&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://cdn2.percipio.com/secure/c/1739516003.dfd1fe81ec26d85052b7349f7f75bee477424294/eot/transcripts/b3ce0952-f71e-4974-a009-84daef8b6b56/it_nlpllmdj_01_enus.html#section_14">15. Video: Course Summary (it_nlpllmdj_01_enus_15)</a></p><p class="toc_link">&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://cdn2.percipio.com/secure/c/1739516003.dfd1fe81ec26d85052b7349f7f75bee477424294/eot/transcripts/b3ce0952-f71e-4974-a009-84daef8b6b56/it_nlpllmdj_01_enus.html#section_15">Course File-based Resources</a></p></div></div><div class="section"><a name="section_0"><h2 class="section_title">1. Video: Course Overview (it_nlpllmdj_01_enus_01)</h2><img src="./Course Transcript_files/image001.jpg" alt="" width="300" onclick="(()=&gt;document.getElementById(&#39;lb-image&#39;).src = &#39;https://cdn2.percipio.com/public/b/ffb62180-310f-42ba-b162-02e3f186a520/image001.jpg&#39;)();"><div class="section_text">In this video, we will discover the key concepts covered in this course.</div><div class="section inner"><h3 class="list_title"></h3><ul class="list"><li class="list_item">discover the key concepts covered in this course</li></ul><div class="section_text">[Video description begins] <em>Topic title: Course Overview. Presented by: Janani Ravi.</em> [Video description ends]
<p>Hi, and welcome to this course on working with tokenizers in Hugging Face. My name is Janani Ravi and I'll be your instructor for this course. Hugging Face is a leading company in the field of artificial intelligence, particularly known for its contributions to natural language processing and machine learning. It offers a comprehensive platform that enables developers and researchers to build, train, and deploy state-of-the-art machine learning models with a strong emphasis on open collaboration and community-driven development.</p>
<p>In this course, you'll learn about the extensive libraries and tools Hugging Face offers, including the transformers library, which provides access to a vast array of pre-trained models and datasets. This knowledge will enable you to explore the Hugging Face platform comprehensively and leverage its capabilities to build, train, and deploy cutting-edge NLP models with ease.</p>
<p>Next, you will set up your working environment in Google Colab, a cloud-based platform that allows for the execution of Python code via a web browser, eliminating the need for powerful hardware. You'll also explore the critical components of the text preprocessing pipeline: normalizers, and pre-tokenizers. These tools are pivotal in preparing text data for efficient processing, ensuring it is in the optimal format for tokenization and enhancing the performance of AI models on NLP tasks.</p>
<p>Finally, you'll master various tokenization techniques, including byte pair encoding (BPE), WordPiece, and Unigram tokenization, which are essential for working with transformer models. Through hands-on exercises, you'll build and train BPE and WordPiece tokenizers, configuring normalizers and pre-tokenizers to fine-tune these tokenization methods. This practical experience will solidify your understanding of tokenization strategies, equipping you with the skills necessary to advance to more complex NLP tasks such as named entity recognition, sentiment analysis, question-answering in upcoming courses.</p></div></div></a></div><div class="section"><a name="section_1"><h2 class="section_title">2. Video: Hugging Face Introduction (it_nlpllmdj_01_enus_02)</h2><img src="./Course Transcript_files/image001(1).jpg" alt="" width="300" onclick="(()=&gt;document.getElementById(&#39;lb-image&#39;).src = &#39;https://cdn2.percipio.com/public/b/8f77cc84-84cc-483a-b96c-0c903e303f4a/image001.jpg&#39;)();"><div class="section_text">After completing this video, you will be able to provide an overview of the Hugging Face platform.</div><div class="section inner"><h3 class="list_title"></h3><ul class="list"><li class="list_item">provide an overview of the Hugging Face platform</li></ul><div class="section_text">[Video description begins] <em>Topic title: Hugging Face Introduction. Presented by: Janani Ravi.</em> [Video description ends]
<p>In this learning path, you'll be working with a number of large language models, models that have millions of parameters. You'll see how you can work with those models directly and you'll also see how you can fine-tune those models for specific natural language processing tasks. Now we'll be accessing all of these large language models and all of them will be transformers using Hugging Face. So, it's important for us to first understand what exactly Hugging Face is all about.</p>
<p>First, let's talk about what exactly Hugging Face is. Hugging Face is actually the name of a company known for its work in the field of artificial intelligence, particularly in natural language processing. Hugging Face has made significant contributions to the AI community through the development and maintenance of a range of open-source libraries, tools, and models that facilitate easy access to state-of-the-art NLP technologies.</p>
<p>The term Hugging Face also applies to the machine learning and data science platform and community that helps users build, deploy, and train machine learning models. Obviously, this is a community fostered by the Hugging Face company. The Hugging Face platform provides the infrastructure to demo, run, and deploy AI in live applications. Users can also browse through models and datasets that other people have uploaded. Hugging Face is often called the GitHub of machine learning because it lets developers share and test their work openly.</p>
<p>So, what can you work with? What are some of the components of the Hugging Face platform? The first and foremost is the transformers library. This is perhaps what Hugging Face is the most famous for. The transformers library provides thousands of pre-trained models for a variety of NLP tasks such as text classification, information extraction, question-answering, summarization, translation, and a whole lot more. These models are based on the transformer architecture and many of these models are freely available for anyone to use.</p>
<p>Along with the transformers library, Hugging Face also offers a high-performance library for tokenization of input text, the process of converting text to a format that can be understood by machine learning models. This library is optimized for speed and efficiency, making it easier to handle large volumes of text. In addition to modeling pools, Hugging Face provides a datasets library which gives users access to a vast collection of natural language processing datasets. This library simplifies the process of downloading, preparing, and loading data, allowing researchers and developers to focus more on model development and less on data handling.</p>
<p>The Hugging Face Hub is an online platform where the community can share and discover models and datasets. It supports collaboration and sharing, making it easy for AI practitioners to find resources for their projects, or contribute their own work to the community. Hugging Face also offers services for hosting machine learning models, making them accessible via an inference API. This allows developers to easily deploy their models and integrate AI functionalities into applications without managing infrastructure.</p>
<p>Hugging Face as a company actively engages in AI research as well often collaborating with academic and industry partners to develop and research natural language processing and other AI models. So, when and why would you choose to use Hugging Face? The first reason, you want to implement machine learning models. Users can upload machine learning models to the platform. There are models available on the platform as well for a variety of functions including natural language processing, computer vision, image generation, audio, and so on.</p>
<p>Hugging Face allows you to share and discover models as well. Through spaces and the Hugging Face transformers library, researchers and developers can share models with the community. Other users can download these models and use them in their applications. The transformers library gives you access to models. Hugging Face Spaces gives you access to applications that users have built using these models. Researchers and developers using Hugging Face can share datasets for training ML models or discover datasets to train their own models, through the datasets library.</p>
<p>Users can access the models available on Hugging Face via the transformers library and fine-tune them on their own datasets for their own specific use cases. Hugging Face makes available infrastructure that you can use for your demos. Users can create interactive in-browser demos of machine learning models, allowing them to showcase and test their models more easily. Hugging Face as a company has been involved in collaborative research projects, but Hugging Face as a platform makes research and collaboration easier as well.</p>
<p>Hugging Face's Enterprise Hub lets business users work with transformers, datasets, and open-source libraries in a privately-hosted environment. And finally, you can use Hugging Face to evaluate ML models. Hugging Face provides access to a code library for evaluating machine learning models as well as datasets. The models available as a part of Hugging Face's transformers library are all, you guessed it, transformers.</p>
<p>Now, the transformer architecture was originally introduced in the 2017 paper, "Attention Is All You Need". The transformer proved to be a groundbreaking innovation in the field of natural language processing. Now transformers introduce the novel concept of attention to focus the model on the right parts of the input. While generating natural language output, attention allows the transformer model to focus on what part of the input sequence is the most relevant to generate different parts of the output. It's safe to say that transformers revolutionize the field of natural language processing and there is a direct line that can be drawn from the transformer architecture from the 2017 paper to the large language models that we work with today.</p>
<p>The GPT models that power ChatGPT are all transformer models. Now thanks to the Hugging Face transformers library, you'll be able to access and work with a wide variety of transformers. The transformers library is a comprehensive open-source library that provides a vast collection of pre-trained models for natural language processing tasks. The transformer models available in this library include BERT, the bi-directional encoder representations from transformers, GPT models, generative pre-trained transformer, the T5 model, text-to-text transfer transformer, and many others.</p>
<p>These models have been pre-trained on large datasets and can be fine-tuned on specific tasks like text classification, question-answering, text generation, and many more. The transformers library is designed to be compatible with both PyTorch and TensorFlow, two of the leading deep learning frameworks. This compatibility allows users to work with their preferred framework without being constrained by the choice of tools, making the library accessible to a broader audience.</p>
<p>The transformers library has been used in a multitude of applications, from developing sophisticated chatbots and virtual assistants to automating content summarization, enhancing search engines, and improving language translation services. Now, if you're going to be training a transformer model from scratch, that's going to be difficult. Transformer models are very large models, and they need to be trained on a huge corpus of data. They have millions of parameters to train and need to be trained on millions of records which means it's going to be expensive in terms of computational resources.</p>
<p>The transformer models in Hugging Face are pre-trained, in that, they've been trained on a large corpus of data and those model ways are available for you to use. These models already understand the nuances and context in language, and you can fine-tune them for your specific use case. Let's say you have a transformer model. You've designed one yourself and you want to train this from scratch. Now pre-training these transformers from scratch, that's a heavy-duty operation. You start with the model weights initialized at random and the model has no prior knowledge of anything. You're essentially feeding in training data so that the model can start learning.</p>
<p>In order for these models to be useful, you have to train using a very large corpus of data and this might require days of training, which means it's going to be expensive. Not all organizations will be able to afford the resources for such a task, which is why it's so useful to have pre-trained transformers available as in the Hugging Face library. Fine-tuning a model involves training a model for a specific task after it has been pre-trained on a large corpus. This allows models to take advantage of knowledge embedded in the original pre-trained model.</p>
<p>For natural language models, the nuances, the meanings, and the context involved in language will be embedded into the pre-trained model. As you'll see in this learning path, fine-tuning a model can be done on a single GPU and is easily reproducible. So, even organizations who don't have a vast amount of resources for model training can make use of fine-tuning to leverage transformer models. By fine-tuning a model rather than training from scratch, you end up using less data, less time, and fewer resources to get good results.</p>
<p>Overall, the cost and the environment impact of training is much lower than when you pre-train a model. When you work with pre-trained models in Hugging Face, you'll use those models via pipelines. Pipelines in Hugging Face is a high-level abstraction that makes it easy for users to perform natural language processing tasks using these pre-trained models. These pipelines encapsulate the entire process of input data, processing it through a model and returning the results, thereby simplifying the use of complex models for specific tasks.</p>
<p>The basic components that make up a Hugging Face pipeline are these three. These work together seamlessly. Tokenizers, models, and then some post-processing. The role of a tokenizer is to convert the raw text into a format that can be understood by the model. This involves splitting the text into tokens and these tokens can be words, sub-words, or characters, and mapping these tokens to numeric IDs. Tokenizers also handle other preprocessing steps like truncating or padding sequences to a uniform length.</p>
<p>Models are, of course, at the core of the pipeline and these are trained on vast amounts of data to perform specific NLP tasks. These models are based on the transformer architecture and there are a wide range of models available, each fine-tuned for specific tasks such as classification, question-answering, text generation, and so on. Users can select models based on the task that they want to accomplish, or even fine-tune existing models on their own datasets for customized applications.</p>
<p>And then, finally, we have post-processing. After the model generates outputs, these are usually in a raw form. Post-processing steps convert these outputs into a human-understandable format. Post-processing might involve converting logits to probabilities for classification or converting token IDs back to text for generation tasks.</p></div></div></a></div><div class="section"><a name="section_2"><h2 class="section_title">3. Video: Hugging Face Tokenizers (it_nlpllmdj_01_enus_03)</h2><img src="./Course Transcript_files/image001(2).jpg" alt="" width="300" onclick="(()=&gt;document.getElementById(&#39;lb-image&#39;).src = &#39;https://cdn2.percipio.com/public/b/66f4db62-3081-4b02-908d-de012e9e6cde/image001.jpg&#39;)();"><div class="section_text">Upon completion of this video, you will be able to outline how tokenization works for transformer models.</div><div class="section inner"><h3 class="list_title"></h3><ul class="list"><li class="list_item">outline how tokenization works for transformer models</li></ul><div class="section_text">[Video description begins] <em>Topic title: Hugging Face Tokenizers. Presented by: Janani Ravi.</em> [Video description ends]
<p>The first thing that we are going to be working within our demos in this learning path are Hugging Face tokenizers. So, let's really understand what tokenizers are all about. Now, we've already discussed the different parts that make up a Hugging Face pipeline. We have the tokenizer, model, and post-processing. The tokenizer you can see is the very first component. Tokenizers are responsible for breaking up the text into smaller sequences, which can be words, subwords, or even characters. Models accept tokenized inputs, and they generate embeddings representations for each of the tokens that allow them to capture the semantic meaning and syntactic relations of the tokens with the surrounding tokens.</p>
<p>And it's thus, by understanding the meaning of the tokens that make up a sentence that the model is able to perform its natural language processing tasks. Let's talk about what tokenization means. Let's say you start off with an input sentence that looks like this. Won't you be going home today? There are different techniques that you can use to tokenize text. For the sake of simplicity, let's assume that we tokenize this text into individual words. Won't you be going home today? So, we have a list of words that make up our sentence. Some tokenizers split on whitespace and keep words together. Other tokenizers may take into account punctuation as well and split out every punctuation character as a separate token. Still, other tokenizers might split words like won't or don't because they actually represent two different words.</p>
<p>As you can see here, there are many different techniques that you can use to tokenize your text. The tokenizers that you've likely encountered so far from the natural language toolkit or spaCy tend to be rule-based tokenizers. Rule-based tokenizers have different rules and they split data in different ways. The tokenizers that we'll use with transformer models are not rule-based tokenizers, instead, they've been trained on a corpus of text data. Before we talk about tokenizers used with transformers, it's important to know that the pre-trained models that we access from the Hugging Face transformers library only work well with inputs that have been tokenized in the same manner as the training data.</p>
<p>There needs to be consistency in how the input data is presented to the model. If the training data was tokenized in a certain manner using a certain tokenization algorithm, when you work with the model in production, you have to tokenize your input in the same way. And this is why all of the models that we'll work within Hugging Face come with their own tokenizer. Let's talk about two broad categories of tokenizers, starting with word tokenizers that split sentences into words. While it's the most intuitive to split text into smaller chunks, this word tokenization method can lead to problems for massive text corpora. When you use space and punctuation tokenization, this usually generates a very big vocabulary, the set of all unique words and tokens used.</p>
<p>Such a big vocabulary size forces the model to have an enormous embedding matrix as the input and output layer, and this can lead to increased memory and time complexity. In practice, transformer models rarely have a vocabulary size greater than 50000 tokens, especially, when they are pre-trained on a single language. At one end of the spectrum, we have the word tokenizer. At the other end, we have character tokenizers which split sentences into individual characters. Character tokenization is very simple and greatly reduces the memory and time complexity of the model. It makes it much harder for the model to learn meaningful input representations because if you think about it, characters by themselves don't have much meaning. Therefore, character tokenization is often accompanied by a loss in performance.</p>
<p>So, what's the alternative? Well, subword tokenization. Subword tokenization lies midway in the spectrum between word tokenization and character tokenization. This relies on the principle that frequently used words should not be split but rare words should be split into subwords. For example, consider the word "emotionally". This could be considered a rare word and this would be split into "emotional" and "ly". Now both emotional and ly would be standalone subwords that would appear more frequently across the text. While at the same time, the meaning of emotionally is kept by the composite meaning of "emotional" and "ly". So, you're not losing the meaning of words but you're also reducing the size of the vocabulary of the tokenizer.</p>
<p>Subword tokenization thus allows the model to have a reasonable vocabulary size while being able to learn meaningful context-independent representations. Subword tokenization also allows the model to process words it has never seen before by decomposing those words into known subwords. The transformer models in the Hugging Phase library use subword tokenization. Now, there are three main types of tokenizers used in transformers. The first is the byte pair encoding tokenizer or BPE. The second is the WordPiece tokenizer, and the third is the Unigram tokenizer. And when you work with transformers in Hugging Face, you'll find that models usually use one of these three tokenizers. Let's understand the basics of each of these tokenization algorithms in turn, starting with byte pair encoding.</p>
<p>The byte pair encoding tokenization algorithm is trained on a corpus of data. Before you feed the corpus of data to the tokenization algorithm, you use a pre-tokenizer to split the training data into words. Once you have all of the words in the vocabulary, you compute the frequency of each unique word in the training data. So, at the end of this process, you have all of the unique words in your training corpus and the associated frequencies. The tokenizer then creates a base vocabulary of symbols for all of the unique words, and it then repeatedly merges these symbols together to create new symbols, and it performs this merging based on merge rules. The symbols or tokens in the base vocabulary are repeatedly merged together to form subword tokens, and the merging is based on merge rules that the algorithm computes.</p>
<p>The merging of symbols in the vocabulary continues till the vocabulary has attained the desired vocabulary size. So, let's say you want a vocabulary of 50000 characters or 40000 characters, the individual tokens will be merged together repeatedly till you reach the vocabulary size of 40000. Now, at every merge step, when two symbols are to be merged together, the byte pair encoding algorithm counts the frequency of each symbol pair and picks the pair that occurs together most frequently. Once the most frequent pair is found, the algorithm looks at all of the words in the training corpus and applies this merge rule to all of the words in the training corpus. These words will now be represented using the merged symbol or merged token rather than the individual tokens. This repeated merging of tokens together to form new tokens continues until the desired vocabulary size is reached.</p>
<p>For example, the GPT model that uses byte pair encoding has a vocabulary size of 40,478 with 478 base characters and 40,000 merges. The next algorithm that we'll discuss is the WordPiece algorithm for tokenization. Now, WordPiece has an algorithm that is very similar to byte pair encoding. It first initializes the base vocabulary to include every character in the training data. The symbols and tokens in the base vocabulary are then merged together using merge rules, and these merge rules are learned from the training corpus itself. What's different about the WordPiece algorithm is how the tokens that are to be merged together is chosen. This algorithm chooses the symbol pair that maximizes the likelihood of the training data once the pair is added to the vocabulary.</p>
<p>In short, at every step, the algorithm evaluates what it stands to lose by merging together two symbols and it ensures that the merge is actually worth it. And the third tokenization algorithm that transformers in Hugging Face use is the Unigram tokenization algorithm. The Unigram algorithm works a little differently from the previous two. It initializes the base vocabulary to a large number of symbols. These symbols include all of the pre-tokenized words and the most common substrings found in the training data. It then progressively trims down each symbol to obtain a smaller vocabulary. The Unigram tokenization thus works a little differently from the BPE tokenization and the WordPiece tokenization. They combine symbols till the desired vocabulary size is reached, whereas the Unigram tokenization trims down symbol to get a smaller vocabulary with the desired size.</p>
<p>At each step in the training process, the Unigram algorithm defines a loss over the training data given the current vocabulary and a Unigram language model. Then for each symbol in the vocabulary, the algorithm computes how much the overall loss would increase if the symbol was to be removed from the vocabulary. This is usually not done per symbol. Unigram usually removes P where the value of P is 10 or 20% of the symbols whose loss increase is the lowest, that is those symbols that least affect the overall loss over the training data. This process is repeated till the vocabulary reaches the desired size. Now the Unigram algorithm typically uses the SentencePiece pre-tokenizer. The SentencePiece pre-tokenizer treats the input as a raw stream and includes the space in the set of characters to use in the vocabulary. The SentencePiece algorithm uses the "_" character to represent the space and this makes decoding of the pre-tokenize output easy. The SentencePiece algorithm is usually used with Unigram tokenizers.</p></div></div></a></div><div class="section"><a name="section_3"><h2 class="section_title">4. Video: Exploring the Hugging Face Platform (it_nlpllmdj_01_enus_04)</h2><img src="./Course Transcript_files/image001(3).jpg" alt="" width="300" onclick="(()=&gt;document.getElementById(&#39;lb-image&#39;).src = &#39;https://cdn2.percipio.com/public/b/faa9bba4-2cad-4cd6-bdd0-859d6b8d6108/image001.jpg&#39;)();"><div class="section_text">Discover how to work with the Hugging Face platform.</div><div class="section inner"><h3 class="list_title"></h3><ul class="list"><li class="list_item">work with the Hugging Face platform</li></ul><div class="section_text">[Video description begins] <em>Topic title: Exploring the Hugging Face Platform. Presented by: Janani Ravi.</em> [Video description ends]
<p>Now that you've been introduced to the Hugging Face platform and the idea of Hugging Face pipelines, let's take a look at how we can set up an account in Hugging Face so that we can use its Models and also push our Models to the Hugging Face hub.</p>
<p>Now, in order to access Hugging Face,</p>
<p>I’ll need to head over to https://huggingface.co/.</p>
<p>That takes me to the main page.</p>
<p>Here we can see that the Hugging Face platform is where the machine learning community collaborates on Models, datasets and applications. [Video description begins] <em>A webpage appears titled: Hugging Face. The menu bar contains the following options: Models, Datasets, Spaces, Posts, Docs, Pricing, Log In, and Sign Up. The main pane displays a banner with the text: The AI community building the future.</em> [Video description ends]</p>
<p>This is definitely true.</p>
<p>Hugging Face has a large number of collaborators and many, many more who use the Models from Hugging Face.</p>
<p>The first step is to Sign Up for a Hugging Face account which is free.</p>
<p>Please note before you practice this on your own, check with your company policies to ensure it is permitted.</p>
<p>Click on the Sign Up option off to the top-right of the screen and specify your Email Address.</p>
<p>Make sure it’s an Email Address that you can access, and then specify the Password that you want to use with this Email Address. [Video description begins] <em>The instructor adds the Email Address as loony.test.001@gmail.com and Password.</em> [Video description ends]</p>
<p>Click on Next and then you’ll be asked for some additional details. Specify your Username, your Full name, any Avatar that you want to use, your GitHub username, and any other details.</p>
<p>Make sure you agree with the Terms and Services, otherwise you won't be able to create your account. [Video description begins] <em>She types loonyhuggingface under Username field, loonyuser under Fullname field, and clicks on the checkbox stating I have read and agree with the Terms of Service and the Code of Conduct.</em> [Video description ends]</p>
<p>Once you're done, go ahead and click on Create Account.</p>
<p>You may be prompted to go through a little security check to prove that you are human.</p>
<p>Well, if you're human, and I hope indeed that you are, then this should be no trouble at all.</p>
<p>Go ahead and respond to this.</p>
<p>Your security check will of course be a little different.</p>
<p>And once you’ve gone and hit Confirm and you’ve got everything right, you should see the Success message.</p>
<p>And that will take you right through to Hugging Face. [Video description begins] <em>A Welcome page opens on the website. Below the menu bar, a banner displays: Please check your email address for a confirmation link. On the right, it has a button: Resend confirmation email.</em> [Video description ends]</p>
<p>Now there is one last thing you need to do. You need to verify your account.</p>
<p>You would have received a confirmation link in your email, so head over to your email Inbox and there you’ll find a confirmation link.</p>
<p>If you don't receive the email right away, go ahead and resend the confirmation email from your Hugging Face account, but you should have the email.</p>
<p>Click on the link here in order to confirm your email address.</p>
<p>And once your email address has been validated, you have complete access to what Hugging Face has to offer.</p>
<p>We'll explore using the links that you see here at the top of your screen. Models, Datasets, and Spaces. [Video description begins] <em>The banner now displays: Your email address has been verified successfully. The menu bar displays the following options: Models, Datasets, Spaces, Posts, Docs, and Pricing. The main pane displays: Join an organization.</em> [Video description ends]</p>
<p>And these are the links that you can use to view the documentation for Hugging Face and all of the Pricing options that you have with Hugging Face.</p>
<p>But for whatever we'll be doing in this learning path, you don't need to have a paid account.</p>
<p>We can use the free account and that will offer us everything that we need.</p>
<p>Let’s start our exploration from the Models page. Hugging Face hosts a large repository of pre-trained Models that are specialized in a wide range of natural language processing tasks.</p>
<p>Tasks such as text classification, question answering, text generation, translation, and so much more.</p>
<p>You can see on the left all of the different categories of Models that Hugging Face hosts.</p>
<p>There are Multimodal Models where you can use different communication modes such as Text, Images, and Video to communicate with Models.</p>
<p>There are Computer Vision Models, and if you scroll further down, you will find that there are Natural Language Processing Models.</p>
<p>That's where we'll be focusing for the rest of this learning path.</p>
<p>And there are also Audio Models, Models that work on Tabular data and Reinforcement Learning Models.</p>
<p>All of these Models that you see here are based on different architectures, such as Google’s BERT, the GPT series of Models GPT-2 and 3, RoBERTa, T5, and many others. [Video description begins] <em>The Models page opens. The left pane displays six options: Tasks, Libraries, Datasets, Languages, Licenses, and Other. Below, it displays a 'Filter Tasks by name' bar. Further, it displays various categories: Multimodal, Computer Vision, Natural Language Processing, Audio, Tabular, and so on. All the categories contain various options. The Natural Language Processing category displays various options. Some of them are: Text Classification, Token Classification, Question Answering, and Summarization. The main pane displays the Models. Besides, it contains a Filter by name bar. It has two buttons: Full-text search and Sort: Trending.</em> [Video description ends]</p>
<p>Collaborators often start with these base Models, fine-tune them for their own use case, and then make them available on Hugging Face.</p>
<p>Let's say that you're interested in a specific task.</p>
<p>Let’s say that’s Text Classification.</p>
<p>All you need to do is click on Text Classification here, and all of the Models that you see here off to the right, will be filtered to show you those Models that support some kind of Text Classification.</p>
<p>Now, if you’re interested in a particular Model, you can simply click on that Model and view additional details.</p>
<p>For example, this ProsusAI/finbert [Video description begins] <em>She selects Text Classification from under the Natural Language Processing category. The main pane displays 47, 691 Models. The one on the top is: ProsusAI/finbert. Below, it shows various details: Text Classification. Updated May 23, 2023. 650k downloads. and 452 likes.</em> [Video description ends] Model is something that I’m curious about.</p>
<p>You can see it has been updated fairly recently in 2023, and it’s quite popular with about 650k downloads.</p>
<p>Let’s click through and what you see here on the main page is the Model card. [Video description begins] <em>The ProsusAI/finbert page opens. It shows various tags: Text Classification, Transformers, PyTorch, TensorFlow, JAX, English, bert, financial-sentiment analysis, and so on. Below, it displays three tabs: Model card, Files and versions, and Community. The Model card tab is active. It displays textual information. On the right, it contains a box: Inference API. It has an input bar with a 'Compute' button below.</em> [Video description ends]</p>
<p>The Model card is usually created by the developers of the Model, and gives you a quick overview of what the Model is exactly about.</p>
<p>It will link to papers and blog posts associated with the Model.</p>
<p>It may also give you additional information, such as how the Model did on different datasets, which are benchmarks.</p>
<p>The Model card is what you look at first to understand what the Model is about.</p>
<p>Another thing I think is really useful is all of those tags that you see right below the name of the Model.</p>
<p>Text Classification, Transformers, PyTorch, TensorFlow, JAX, English, bert.</p>
<p>This tells you a lot more about the Model itself. It’s a Transformer Model.</p>
<p>It can be used with Hugging Face in PyTorch, TensorFlow or JAX.</p>
<p>You can see this in the Model card as well.</p>
<p>It’s a fine-tuned version of the BERT language model tweaked to work in the finance domain.</p>
<p>If you click on the Files and versions tab, this will give you access to the repository of the Model, not the Model code, because this is the BERT Model after all, but you'll get access to the configuration associated with the Model as well as the Model weights. [Video description begins] <em>The Files and versions tab is active. It contains the following buttons: main, finbert, History, and Contribute. Below, it contains a table with various files in different formats.</em> [Video description ends]</p>
<p>If you click on the Community tab, you'll find all of the collaborators who work on the Model and all of the users of this Model as well.</p>
<p>Discussions, Pull requests, and a bunch of other interesting details. [Video description begins] <em>The Community tab is active. On the left it displays: New discussion, New pull request, and Resources. The main pane displays three tabs: All, Discussions, and Pull requests.</em> [Video description ends]</p>
<p>If you're going to be using this Model, you’ll definitely want to be part of the Community.</p>
<p>Let's go back to the Model card and let me show you something interesting here.</p>
<p>Many of the Models hosted on Hugging Face offer an Inference API that allows you to provide an input to the Model and get a response from the Model right here from the Hugging Face UI.</p>
<p>Now, all of the statements are going to be about the financial markets.</p>
<p>The first one says, “The prospect of a Fed rate cut had a galvanizing effect on the markets”. Click on Compute and you can see that this statement is clearly positive. [Video description begins] <em>She inputs the text in the Inference API input bar. Below, it displays three metrics with a bar scale: positive, neutral, and negative.</em> [Video description ends]</p>
<p>Let's try this once again with another statement.</p>
<p>“Overall liquidity was tight and the markets reflected that.”</p>
<p>This is clearly a negative statement.</p>
<p>Click on Compute and let’s see what the Model has says.</p>
<p>And you can see that it categorizes this as negative.</p>
<p>Let’s try this once again, “Stocks were buoyant on positive economic news but worries remain”.</p>
<p>Now here you can see that the Model has categorized this as mostly negative.</p>
<p>Negative has a higher score than positive.</p>
<p>But overall this is not as negative as the previous statement.</p>
<p>Let’s go back to Models and explore a few more Models.</p>
<p>Here on the left-hand side, let me look at another NLP Model.</p>
<p>This time I’m interested in Summarization. Click on Summarization.</p>
<p>And here you can see some of the most popular Models that can be used for summarizing text.</p>
<p>Let’s scroll back up and click through to the very first Model, facebook/bart - large - cnn.</p>
<p>This is a Model that Facebook has developed. [Video description begins] <em>The facebook/bart-large-cnn page opens. It shows various tags: Summarization, Transformers, PyTorch, TensorFlow, JAX, Rust, Safetensors, and so on. Below, it displays three tabs: Model card, Files and versions, and Community. The Model card tab is active. It displays textual information.</em> [Video description ends]</p>
<p>This is a BART model pre-trained on the English language and fine-tuned on the CNN Daily Mail dataset.</p>
<p>BART is a Sequence-to-Sequence Model originally released in 2019 for Text Generation, Translation, and Comprehension.</p>
<p>The Model card gives you many more details on how to use BART in the pipeline API, what BART is trained on, and so on.</p>
<p>We’ve explored Models, now let’s move on to Datasets.</p>
<p>Hugging Face provides a vast collection of Datasets which cover a broad spectrum of NLP tasks and languages, which facilitate research and development in machine learning.</p>
<p>On the left, you can see the different categories of Datasets. Multimodal Datasets, Computer Vision Datasets, Datasets for Natural Language Processing, Datasets for Audio, for Tabular, and so on.</p>
<p>Let’s say you’re interested in training a Question Answering Model. [Video description begins] <em>The Datasets option is active. On the left, it displays the categories. The main pane displays the Datasets.</em> [Video description ends]</p>
<p>You might click through to Question Answering, and you’ll get all of the Datasets filtered based on your use case.</p>
<p>One of the most popular Datasets for question answering is the squad_v2 dataset.</p>
<p>It's a Stanford University dataset, and you can see that that's available here on Hugging Face as well. [Video description begins] <em>A Datasets: squad_v2 page opens. Below, it displays various Tasks and Language Creators. Further, it contains three tabs: Dataset card, Files and versions, and Community. The Dataset card tab is active. It displays: Dataset Viewer. Below, it contains a drop-down bar which displays: train (130k rows).</em> [Video description ends]</p>
<p>If you scroll down, you get a nice view of what exactly this Dataset looks like.</p>
<p>You can see here that we have 130k records in the training data, and there are about 11.9k records in the validation data.</p>
<p>Let’s go back to Datasets.</p>
<p>And let's say you're interested in a different kind of problem.</p>
<p>Well use the filtering mechanism to search for a dataset for that particular type of problem.</p>
<p>Let’s say you select Text Classification and here you can see all of the Datasets available on Hugging Face for the Classification problem.</p>
<p>And you can pick and choose the right one for your use case.</p>
<p>If you select the imdb Dataset, this seems to be the most popular.</p>
<p>You can view the details right here.</p>
<p>We have the text and the corresponding label representing a sentiment class [Video description begins] <em>The imdb Dataset page opens. The Dataset card tab is active. It displays: text and label.</em> [Video description ends] or category.</p>
<p>This dataset contains three subsets of Data, training, test, both 25k records and an unsupervised 50k records. [Video description begins] <em>Three options appear in the drop-down bar. They are: train (25k rows), test (25k rows) and unsupervised (50k rows).</em> [Video description ends]</p>
<p>Finally, let’s explore Hugging Face Spaces.</p>
<p>Spaces is a platform provided by Hugging Face that allows developers and researchers to deploy and share their machine learning applications.</p>
<p>These applications can leverage the Models and Datasets available on Hugging Face allowing users to interact with AI Models through user friendly interfaces. [Video description begins] <em>The Spaces option is active. It displays a 'Search Spaces' bar. Further, it contains various apps.</em> [Video description ends]</p>
<p>Now, let me show you a particular application here, which was one of the trending applications the week this course was recorded, Screenshot to HTML.</p>
<p>I’m going to click through and you’ll see a nice little user interface that you can use to interact with the Model behind this app.</p>
<p>This app allows you to upload a Screenshot here on the top-left of your screen, and it will try and figure out the HTML that was used to create that particular Screenshot. [Video description begins] <em>The screenshot2html app page opens. On the top right, it displays three options: App, Files, and Community. The main pane displays three sections. On the left it displays: Screenshot to extract. It has a 'Drop Image here or Click to Upload' option. Further, it has three buttons: Submit, Clear, and Regenerate. On the right, it displays: Rendered HTML. Below, it displays: Extracted HTML.</em> [Video description ends]</p>
<p>I have a Screenshot of a portion of the Hugging Face web interface on my local machine.</p>
<p>HF_screenshot.png.</p>
<p>I'm going to select this and upload this to the application, and see whether it can figure out the HTML that corresponds to this particular rendering of a web page.</p>
<p>You can see from the messages that the Model running behind the scenes uses a GPU in order to figure out the HTML and stylesheet corresponding to this page.</p>
<p>You can see here at the bottom that it first outputs the CSS styles that may be used to get the exact rendering of this page.</p>
<p>And here at the bottom you can see the HTML has been generated as well.</p>
<p>Now, I'm not sure how good or perfect this generated HTML is.</p>
<p>I didn't run it in a browser to find out, but you have to admit that it contains the important elements.</p>
<p>It contains all of the right bits of text and some styles which all seem very sensible to me.</p>
<p>I'll leave it to you to try this app out on your own.</p></div></div></a></div><div class="section"><a name="section_4"><h2 class="section_title">5. Video: Setting up the Colab Environment (it_nlpllmdj_01_enus_05)</h2><img src="./Course Transcript_files/image001(4).jpg" alt="" width="300" onclick="(()=&gt;document.getElementById(&#39;lb-image&#39;).src = &#39;https://cdn2.percipio.com/public/b/f821a66d-8d6c-4b93-b75d-43604e545240/image001.jpg&#39;)();"><div class="section_text">Learn how to set up a Colab notebook.</div><div class="section inner"><h3 class="list_title"></h3><ul class="list"><li class="list_item">set up a Colab notebook</li></ul><div class="section_text">[Video description begins] <em>Topic title: Setting up the Colab Environment. Presented by: Janani Ravi.</em> [Video description ends]
<p>Before we get started working with Hugging Face pipelines, let's get set up with the environment that we'll use to build and train our models. We'll write all of our code in Google Colab. Head over to colab.research.google.com. Google Colab, short for Colaboratory, is a free cloud service hosted by Google that allows anyone to write and execute Python code via the browser. Colab offers cloud-hosted notebooks that comes pre-installed with a number of libraries in Python for data science and machine learning. Colab also provides free access to GPUs and TPUs which can significantly accelerate the training of machine learning models.</p>
<p>In order to use Colab, all you need to do is Sign In with your Google account, whether it's a Gmail account or your organization's Google account. Once you've signed in, you can create Colab notebooks, which is just a Jupyter Notebook environment which gives you an interactive computing framework that you can use to run your Python code. I click on New notebook to create a brand-new notebook <br><br>[Video description begins] <em>A page titled 'Welcome to Colaboratory' appears. Currently, a dialog box named Open notebook is open within the page. On the left-hand side, it displays multiple sections: Examples, Recent, Google drive, and Upload. At the bottom-left, it contains a New notebook button.</em> [Video description ends]<br><br>within Colab, and you can click on this + Code button <br><br>[Video description begins] <em>A Colab notebook appears named Untitled3.ipynb. At the top, the menu bar displays various options namely, File, Edit, View, Insert, Runtime, and so on. The left pane displays various icons including a key icon, a folder icon, and more. Below, it displays a code cell. The notebook also includes two tabs: +Code and +Text.</em> [Video description ends]<br><br>to generate more code cells. Shift+Enter will also create code cells as needed. Let's rename this notebook to have a meaningful name Normalization_Pre_tokenization because these are the operations that we're going to be performing. Your Colab notebook can use a number of different hardware runtimes. If you click on Runtime and go to Change runtime type, this will bring up a dialog showing you what Hardware accelerators you have available.</p>
<p>You can also use this little drop-down to see in what <br><br>[Video description begins] <em>The Change runtime type dialog box displays two sections: Runtime type and Hardware accelerator. The Runtime type section has a drop-down field with 2 options: Python 3 and R. The Hardware accelerator section displays 5 radio button options. Some of the names are: CPU, T4 GPU, and TPU</em> [Video description ends]<br><br>languages you can code. Python 3 and R are currently available. We'll stick with Python 3. Now notice that amongst the hardware accelerators, I have the CPU selected and that's totally fine. There is a T4 GPU available to me but I don't really need it for this demo so I'm not going to use it. We are going to be accessing tokenizers, models, and pipelines from Hugging Face and in order to be able to connect to Hugging Face from our Colab notebook, we'll need to generate a secret access token on Hugging Face and configure that token in our notebook environment.</p>
<p>Head over to huggingface.co and click on the little profile icon off to the top right of your screen. <br><br>[Video description begins] <em>A drop-down menu appears with the various options namely, Inbox, Settings, and Sign Out.</em> [Video description ends]<br><br>This will bring up a little drop-down. Head over to Settings and this will take you to your account settings on Hugging Face. Here you'll find your username and a bunch of other details, <br><br>[Video description begins] <em>A section named Profile Settings opens. The left pane displays several options: Profile, Account, Organizations, Billing, Access Tokens, and many more.</em> [Video description ends] <br><br>but what we're interested in is on the left here, Access Tokens. Hugging Face Access Tokens allow you to authenticate yourself to Hugging Face when you're accessing models, datasets, or pipelines from Hugging Face. You can set up tokens to have read, write, or admin access. We don't really need admin access to Hugging Face. We'll set up a token that grants write access to our Hugging Face account. I've called this token loony-access-token and I use the drop-down <br><br>[Video description begins] <em>The Access Tokens section opens. It contains the New token button. As she clicks the button, a dialog box appears named Create a new access token. It has two fields: Name and Role. It also has a Generate a token button.</em> [Video description ends]<br><br>to select the Role that I want this token to have. This will be the write role.</p>
<p>Once you've made your selection, click on Generate a token and a new token will have been created for your account. I'm now going to copy this token over and configure this token in my Colab environment. <br><br>[Video description begins] <em>The Access Tokens section now contains a token under a heading loony-access-token.</em> [Video description ends]<br><br>Once you've copied it over, let's head back to Colab where we've just set up a notebook, and let's configure this Hugging Face secret access token in our Colab environment. Click on that little security icon that you see with the key off to the left sidebar and this is where you configure your Secrets within the Colab environment. Here below you can also see the code that you can use to access <br><br>[Video description begins] <em>A panel appears titled Secrets. It contains a button: Add new secret. At the bottom, it displays the following code: from google.colab import userdata userdata.get('secretName').</em> [Video description ends] <br><br>your secrets in Python if you need to. All we have to do here is click on Add new secret and this will bring up a little text box where we can specify the name of our secret token. [Video description begins] <em>A table appears within the panel. It contains the following column headers: Notebook access, Name, Value, and Actions.</em> [Video description ends]</p>
<p>Now in order to connect to Hugging Face, the name of your secret token has to be HF_TOKEN all in uppercase. Any interaction that you have programmatically with Hugging Face will automatically look for this token. Now make sure you configure the value of this token to the token that you just generated for your Hugging Face account. Another thing I enable here is Notebook access so that this token will be accessible from the notebook. Though, it's not really something that we need. Once you've completed this configuration, any time, your Notebook accesses models or pipelines or tokenizers, or anything from Hugging Face, it'll make use of this token.</p>
<p>Just two things to note, at the time of this recording, it wasn't required to have the HF_TOKEN secret access token configured in your Colab environment. It only threw up a warning, but it allowed you to access models and datasets from Hugging Face anyway. However, in the future, this warning could become an error, which is why I thought it makes sense to show you how you configure the Hugging Face token for your Colab environment. Another thing to note here is that once you configure the secret access token for one notebook, this is actually available across the entire Colab runtime for this particular account so all your other notebooks will be able to use the same token. At this point, we have our environment all set up and we are ready to get started with code.</p></div></div></a></div><div class="section"><a name="section_5"><h2 class="section_title">6. Video: Normalizers and Pre-tokenizers (it_nlpllmdj_01_enus_06)</h2><img src="./Course Transcript_files/image001(5).jpg" alt="" width="300" onclick="(()=&gt;document.getElementById(&#39;lb-image&#39;).src = &#39;https://cdn2.percipio.com/public/b/12ede4c7-44e0-40ea-9c5e-23b1c16d96a2/image001.jpg&#39;)();"><div class="section_text">In this video, we will explore normalization and pre-tokenization.</div><div class="section inner"><h3 class="list_title"></h3><ul class="list"><li class="list_item">explore normalization and pre-tokenization</li></ul><div class="section_text">[Video description begins] <em>Topic title: Normalizers and Pre-tokenizers. Presented by: Janani Ravi.</em> [Video description ends]
<p>Before text can be used for natural language processing, there is a bunch of preprocessing operations that you have to apply so that the text is in a form that can be used to train a machine learning model. One of these preprocessing operations is tokenization, which is the process of converting a sequence of text into discrete units called tokens, and these discrete units can be individual words, characters, or sub-words. This step is fundamental in NLP as it transforms raw text into a format that can be analyzed and used by machine learning models. When you use Hugging Face models, there are two critical steps that need to be performed before tokenization, and those steps are normalization and pre-tokenization, and that's what we're going to be looking at in this demo.</p>
<p>The first of these steps is normalization, which is the process of converting the text into a more uniform format. This step helps in reducing the complexity of the text data and in handling the variability in language use. Normalization can involve lower casing, removing accents and diacritics, converting to unicode, and maybe even whitespace and punctuation normalization. Normalization comprises of a set of operations that you apply to a raw string to make it less random or cleaner. The second operation is pre-tokenization, which involves splitting the normalized text into a preliminary set of tokens or words. This step breaks down the text into manageable pieces but does not yet involve understanding the sub-token structure that the model requires. Pre-tokenization typically involves splitting on whitespace or splitting on punctuation.</p>
<p>The goal of pre-tokenization is to create a basic token structure that reflects the natural divisions in text, preparing it for the more model-specific sub-tokenization step that follows. The output of the pre-tokenization step is what is fed into the tokenizer, and tokenizers tend to be model-specific. Now with this understanding, let's go ahead and set up the imports that we need. [Video description begins] <em>A page titled Normalization_Pre_tokenization.ipynb is open. Line 1 reads: from tokenizers import normalizers. Line 3 reads: from tokenizers.normalizers import NFD, StripAccents, Lowercase.</em> [Video description ends] I'm going to import normalizers and from tokenizers.normalizers, I import three different normalization operations, NFD, StripAccents, and Lowercase. NFD stands for Normalization Form Decomposed and is a kind of unicode normalization that decomposes combined characters in the string into their constitute components. Notice that I'm setting up these imports, but I haven't pip installed any Python library.</p>
<p>That's because the Hugging Face Transformers Library is available in the Colab environment. If you do need to install this library for any reason, all you have to do is a pip install transformers and restart the runtime if needed. Now normalizers can be applied one at a time or in a sequence. [Video description begins] <em>Line 1 reads: normalizer = normalizers.Sequence([Lowercase()] ). Line 3 reads: normalizer.normalize_str("Cafe culture is prominent in many cities around the world.").</em> [Video description ends] Here I've instantiated a class normalizers.Sequence that takes in a sequence of normalizers. I've specified just one though the Lowercase normalizer, and once I have the normalizer, I call normalizer.normalize_str and pass in the string that I want to normalize, and let's see what the result looks like. The Lowercase normalizer, as the name suggests, converts the entire string so that all of the characters are in the lowercase, and you can see that in the result, lowercasing is the only normalization operation that we've performed.</p>
<p>Let's try this once again, and this time I'm going to specify a sequence of normalizers. [Video description begins] <em>Line 1 reads: normalizer = normalizers.Sequence([NFD(), StripAccents(), Lowercase()] ).</em> [Video description ends] The NFD normalizer, followed by the StripAccents normalizer, followed by the Lowercase normalizer. These normalization operations will be performed one after the other. I call normalizer.normalize_str and pass in the same sentence as before. Cafe culture is prominent in many cities around the world and you can see the little accent above the e for cafe. Here's what the result looks like. The NFD normalizer, the character e with the accent on top, would be decomposed into a regular e and a separate accent character.</p>
<p>But StripAccents get rid of the separate accent character and Lowercase converts the entire string into lowercase, and this is how we get the result that you see here on screen. Let's try the same sequence of normalizers again. The NFD, StripAccents, and Lowercase with another example. The protagonist had deja vu when he entered the old mansion. <br><br>[Video description begins] <em>Next cell reads: normalizer.normalize_str("The protagonist had déjà vu when he entered the old mansion.").</em> [Video description ends] <br><br>The e and the a in deja vu both have accents. The NFD normalizer would have separated the accent from the base character so we would be left with the e and the a, and then StripAccents got rid of the accents, and Lowercase converted the entire text to lowercase. Let's try another example with many accents and you can see a number of these accents are just gratuitous. <br><br>[Video description begins] <em>Next cell reads: normalizer.normalize_str("Hèllò hôw are ü?").</em> [Video description ends]<br><br>Hello, how are you with a bunch of different accents? All of the accents are removed and the text is in lowercase. And really this is all there is to normalization.</p>
<p>You pick the kind of normalization you want to apply to your input text and go ahead and set them up in the form of a sequence and apply the normalizer. The tokenizers that you'll use in Hugging Face have their own pre-built normalizers so most of the time you won't directly be interacting with normalizers at all but it's important for you to know that they exist and you can actually access and configure them as you need. Let's move on from normalization and take a look at pre-tokenization. The Hugging Face Transformers library offers a number of different pre-tokenizers which you can string together as a sequence. The first pre_tokenizer that we are going to look at is the Whitespace pre_tokenizer which splits input text based on the whitespace. I instantiate the pre tokenizer on line 3 <br><br>[Video description begins] <em>Line 3 reads: pre_tokenizer = Whitespace(). Line 5 reads: pre_tokenizer.pre_tokenize_str("She can't attend the meeting due to prior commitments.").</em> [Video description ends]<br><br>and I call pre_tokenizer.pre_tokenize_str and pass in a sentence.</p>
<p>She can't attend the meeting due to prior commitments. If you take a look at the result here, you can see the tokens into which the sentence has been split. She, can, then the quote, and then the t, and attend the. So, the split is based on the whitespace but punctuation and special characters are also split into separate tokens. The tokens are generated as a list of tuples. The first element of the tuple is the token itself, the second element is a tuple specifying the range in the input text where that token occurs. Now you may want to split your input text using the whitespace but you may also want to split on digits. Here I have specified a pre_tokenizer.Sequence on line 4. <br><br>[Video description begins] <em>Line 1 reads: from tokenizers import pre_tokenizers. Line 2 reads: from tokenizers.pre_tokenizers import Digits. Line 4 reads: pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits = False)]).</em> [Video description ends]</p>
<p>The first pre_tokenizer will be the Whitespace which will split the input text on the Whitespace and then we further split the tokens on Digits and I have said individual_digits = False, meaning if there are a combination of digits, they'll all be treated as a single token. On line 6, I call pre_tokenizer.pre_tokenize_str and the sentence is, I'm calling you on and there is a phone number there which has multiple digits. <br><br>[Video description begins] <em>Line 6 reads: pre_tokenizer.pre_tokenize_str("I am calling you on 93457654").</em> [Video description ends] <br><br>Let's see what the result looks like. You can see each individual word is a separate token and all of the digits come together in a single token.</p>
<p>This is because we specified individual_digits = False. Let me go ahead and change that. Instead of False, I'm going to set individual_digits = True and rerun this code cell. Observe how the individual digits of the phone number are broken into separate tokens. Now what we've just discussed is just a small taste of what you can do with normalizers and pre_tokenizers. I'll leave it to you to explore this in more detail.</p></div></div></a></div><div class="section"><a name="section_6"><h2 class="section_title">7. Video: Byte Pair Encoding (BPE), Wordpiece, and Unigram Tokenization (it_nlpllmdj_01_enus_07)</h2><img src="./Course Transcript_files/image001(6).jpg" alt="" width="300" onclick="(()=&gt;document.getElementById(&#39;lb-image&#39;).src = &#39;https://cdn2.percipio.com/public/b/42b00280-9387-455d-885e-1fcf8a9d1e78/image001.jpg&#39;)();"><div class="section_text">During this video, you will learn how to perform byte pair encoding (BPE) and WordPiece tokenization.</div><div class="section inner"><h3 class="list_title"></h3><ul class="list"><li class="list_item">perform byte pair encoding (BPE) and WordPiece tokenization</li></ul><div class="section_text">[Video description begins] <em>Topic title: Byte Pair Encoding (BPE), Wordpiece, and Unigram Tokenization. Presented by: Janani Ravi.</em> [Video description ends]
<p>Different models in the Hugging Face Transformers library use different tokenizers. Now in the same demo where we looked at normalization and pre-tokenization, I'm going to give you a quick introduction to the different tokenizers used by different models in the Hugging Face library. Now the first tokenizer that we're going to look at is the tokenizer used by the BERT model. <br><br>[Video description begins] <em>A web page titled Hugging Face opens. At the top, it contains a search bar and multiple tabs. The names of the tabs are: Models, Datasets, Spaces, Posts, Docs, and Pricing.</em> [Video description ends]<br><br>Now if you want to look at the details of a certain model, head over to Hugging Face and select the Models tab here. Here you can search for a specific model that you're interested in bert-base-uncased. This is the model whose tokenizer I'm going to look at. I search for the model and the first hit is the model itself and I select and look at the model card.</p>
<p>BERT stands for Bidirectional Encoder Representations <br><br>[Video description begins] <em>A corresponding page opens with the heading: bert-base-uncased. It contains 3 tabs: Model card, Files and versions, and Community. The Model card tab is active. It displays information under the heading: BERT base model (uncased).</em> [Video description ends]<br><br>from Transformers and is a method of pre-training language representations introduced by Google in 2018. This is a model that has significantly advanced the state-of-the-art in a wide range of natural language processing tasks. The BERT model is bidirectional and this is what allows the model to understand the context of a word based on all of its surrounding text both to the left and to the right of the word. You can see the details of the BERT base model here in this model card. This is the model whose tokenizer we are going to look at first. It's an uncased model because it does not differentiate between words based on its casing. Now that we know little about the model whose tokenizer we are about to access and use, let's head back to our notebook and I'll show you how you can load in the bert-base-uncased tokenizer.</p>
<p>The BERT model uses a WordPiece tokenizer. The WordPiece tokenizer starts with a small vocabulary and repeatedly merges the tokens in the vocabulary together to create sub-words. Observe on line 3 how I use the AutoTokenizer class to load in a pre-trained tokenizer. AutoTokenizer.from_pretrained and I specify the name of the model whose tokenizer I am interested in, bert-base-uncased. <br><br>[Video description begins] <em>A notebook page opens with the heading: Normalization_Pre_tokenization.ipynb. The following line of code is highlighted: tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased").</em> [Video description ends]<br><br>AutoTokenizer is a powerful tool for automatically loading and using the correct tokenizer class associated with the pre-trained model. It simplifies your workflow by eliminating the need to manually identify and import the specific tokenizer needed for your chosen model. Once I have the BERT tokenizer, I call tokenizer.tokenize and pass in some input text. I have a new SAMSUNG GLITE. <br><br>[Video description begins] <em>Line 5 reads: tokenizer.tokenize("I have a new SAMSUNG GLITE").</em> [Video description ends]</p>
<p>Notice that the tokenizer is downloaded from the Hugging Face library and you can see the resulting tokens. Tokenizers used by transformers are generally sub-word tokenizers and here you can see that the individual tokens are, i, have, a, new, samsung, g is a separate token, lite is a separate token. The double hash that you see before lite means that the rest of the token should be attached to the previous one without a space whenever you need to decode or reverse the tokenization process. When a word is broken into sub-tokens, the first sub-token is just the sub-token by itself. Sub-tokens that follow the first are generally preceded by the double hash in the WordPiece tokenization algorithm. Let's use the WordPiece tokenizer used by BERT to tokenize some other text.</p>
<p>Hello, y'all! How are you? Notice the double space in this sentence. Here are what the resulting tokens look like. <br><br>[Video description begins] <em>Next cell reads: tokenizer.tokenize("Hello, y'all! How are you 😄?").</em> [Video description ends] <br><br>A few things to observe punctuation characters, such as the comma, the question mark are separate tokens. The double space between how and are is ignored, and that little emoji we have there, that is the unknown token UNK that you see. The emoji is clearly not part of WordPieces vocabulary for BERT. The BERT tokenizer gives you access to the pre-tokenizer for BERT using the backend_tokenizer member variable. This is the actual tokenizer which performs the tokenization. <br><br>[Video description begins] <em>Next cell reads: tokenizer.backend_tokenizer.</em> [Video description ends]<br><br>And if you access tokenizer.backend_tokenizer.pre_tokenizer, that gives us the pre-tokenizer for BERT. I call pre_tokenize_str and I pass in, I have a new SAMSUNG GLITE and this gives us the pre-tokenized output. <br><br>[Video description begins] <em>Next cell reads: tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("I have a new SAMSUNG GLITE").</em> [Video description ends]</p>
<p>You can see that the original set of tokens has been broken down into individual words. This pre-tokenized output is what the WordPiece tokenizer for BERT uses in order to represent this input text as sub-word tokens. Let's look at the pre-tokenized output for the other sentence we were working with. Hello, y'all! How are you with a double space between How and are? Here is the output of the pre-tokenizer which contains all of <br><br>[Video description begins] <em>Next cell reads: tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, y'all! How are you 😄?").</em> [Video description ends]<br><br>the individual words in the sentence and the punctuations are separated into separate tokens as well. The pre-tokenizer ignores the two spaces between how and are but in the tokens generated and the corresponding offsets, notice the offset for are is 20 which takes into account the double space between how and are. Observe that when we only use pre-tokenization, the output is not normalized.</p>
<p>The normalizer for the BERT tokenizer can also be accessed via the backend tokenizer, tokenizer.backend_tokenizer.normalizer <br><br>[Video description begins] <em>Next cell reads: tokenizer.backend_tokenizer.normalizer.normalize_str("The protagonist had déjà vu when he entered the old mansion.").</em> [Video description ends] <br><br>gives us access to the normalizer and I invoke the normalize_str function and I pass in a sentence and this will allow you to see the normalization that is applied to the input before it's passed on to the BERT tokenizer. The BERT tokenizer, as we know, is the WordPiece tokenizer. The normalization has converted the input text to lowercase and has stripped all of the accents from the input. Let's apply the normalizer to another example here. This is the "Hello how are u?", with a lot of Greto it is accents and here you can see all of the accents have been stripped out, but the base character has been preserved and the output is entirely in lowercase.</p>
<p>We were briefly introduced to the WordPiece tokenizer that the BERT model uses. Next, we'll look at the tokenizer the GPT-2 uses this is the byte pair encoding tokenizer. <br><br>[Video description begins] <em>Line 1 reads: tokenizer = AutoTokenizer.from_pretrained("gpt2"). Line 3 reads: tokenizer.tokenize("I have a new SAMSUNG GLITE").</em> [Video description ends]<br><br>Byte pair encoding is used by the GPT series of models including GPT, GPT-2, RoBERTa, Bart, and DiBerta. Byte pair encoding uses the training data to generate a base vocabulary which usually starts off with individual characters and this base vocabulary tokens are then merged together till the desired vocabulary size is reached. In order to access the byte pair encoding tokenizer that GPT-2 uses, I call AutoTokenizer.from_pretrained and pass in the name of the model gpt2. I call tokenizer.tokenize and I pass in the same sentence.</p>
<p>I have a new SAMSUNG GLITE and this is what the tokenized output looks like. This tokenizer splits on the whitespace and punctuation, but it keeps the spaces and replaces them with that strange G symbol, G with a little dot on top. This allows us to recover the original spaces if we decode the tokens. Let's try the GPT-2 tokenizer with our other sentence. Hello, y'all! How are you with two spaces between how and are. Once again, we can see that the words are split on the whitespace <br><br>[Video description begins] <em>Next cell reads: tokenizer.tokenize("Hello, y'all! How are you 😄?").</em> [Video description ends] <br><br>and punctuation characters and the spaces are replaced by that strange little G symbol. Notice that the double whitespace is not ignored here. We have the two consecutive G symbols representing the double whitespace. As in the case of the Bert tokenizer, we can use the backend_tokenizer variable to access the pre-tokenizer. <br><br>[Video description begins] <em>Next cell reads: tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("I have a new SAMSUNG GLITE").</em> [Video description ends]<br><br>Let's pre-tokenize this sentence.</p>
<p>I have a new SAMSUNG GLITE and you can see that the pre-tokenization splits on the whitespace and uses that G symbol to represent the spaces in our text. Let's pre-tokenize the other sentence as well, the one with the double space between How and are and here you can see the output of the pre-tokenization step. Let's look at the third type of tokenizer that natural language models on Hugging Face use and that is the Unigram tokenizer. <br><br>[Video description begins] <em>Line 1 reads: tokenizer = AutoTokenizer.from_pretrained("t5-small"). Line 3 reads: tokenizer.tokenize("I have a new SAMSUNG GLITE").</em> [Video description ends] <br><br>We've discussed that the Unigram tokenizer uses the SentencePiece algorithm. Now the t5-small model uses this tokenizer and I call AutoTokenizer.from_pretrained("t5-small") to load in the tokenizer from this model from Hugging Face. I call tokenizer.tokenize, I have a new SAMSUNG GLITE. The tokenizer weights are loaded. Let's look at the resulting tokens.</p>
<p>Like the GPT-2 tokenizer, this one keeps the spaces that occur in the input text and replaces them with a specific token. This is the underscore. The t5 tokenizer only splits on whitespace, not on punctuation. Also, note that it adds a space by default at the beginning of the first token so we have _I rather than just I. Let's try tokenizing the sentence which has the emoji and the double space between How and are, tokenizer.tokenize. Let's look at what the SentencePiece algorithm does with this. Observe that the word are has just one underscore before it. The double space was ignored.</p>
<p>Also note that the first word Hello had a space added before it by default and you can see the underscore representing the space before the emoji as well. Once again, you can use the backend_tokenizer variable to access the pre-tokenizer for the SentencePiece algorithm. Here is the output of the pre-tokenizer, which is then shared <br><br>[Video description begins] <em>Next cell reads: tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("I have a new SAMSUNG GLITE").</em> [Video description ends]<br><br>into the tokenization algorithm, the Unigram tokenizer which uses the SentencePiece algorithm. Let's go ahead and try and pre-tokenize this second sentence as well so we can see what the result looks like. <br><br>[Video description begins] <em>Next cell reads: tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str("Hello, y'all! How are you 😄?").</em> [Video description ends]<br><br>Note that the pre-tokenizer does not split on punctuation, only on the space.</p></div></div></a></div><div class="section"><a name="section_7"><h2 class="section_title">8. Video: Implementing Byte Pair Encoding Tokenization - I (it_nlpllmdj_01_enus_08)</h2><img src="./Course Transcript_files/image001(7).jpg" alt="" width="300" onclick="(()=&gt;document.getElementById(&#39;lb-image&#39;).src = &#39;https://cdn2.percipio.com/public/b/e05d2244-938a-4d6f-8a60-c71cfdb7e123/image001.jpg&#39;)();"><div class="section_text">Find out how to set up a BPE tokenizer.</div><div class="section inner"><h3 class="list_title"></h3><ul class="list"><li class="list_item">set up a BPE tokenizer</li></ul><div class="section_text">[Video description begins] <em>Topic title: Implementing Byte Pair Encoding Tokenization - I. Presented by: Janani Ravi.</em> [Video description ends]
<p>In this demo, we'll understand the nuances of subword tokenization in more detail by looking at an implementation of byte pair encoding tokenization. This will be an actual implementation of the PPE algorithm so we are really getting into the weeds of subword tokenization. Just a heads up that this implementation is present in the original Hugging Face documentation. The code that we are about to see here has a few modifications for clarity. Our implementation will not be an optimized version that you can use for real on a large production-scale training corpus but it will be sufficient for you to understand the exact algorithm behind how this tokenization occurs. Let me give you the big-picture overview of the steps involved in BPE tokenization.</p>
<p>We'll first set up a base vocabulary which will comprise of all of the characters in our training corpus. We'll then use the frequency of pairs of tokens to determine how to merge tokens together to get to our final vocabulary size. We'll keep merging till we reach the vocabulary size we are interested in. Let's go ahead and set up the corpus. This will be our training corpus for the tokenizer. <br><br>[Video description begins] <em>A Colab notebook titled Implementation_BPETokenizer.ipynb is open. The first cell displays multiple lines of code. Line 1 reads: corpus = [. Line 2 reads: "This is a Natural Language Processing course.",. The rest of the lines display a similar pattern of code.</em> [Video description ends]<br><br>You can see there are four different sentences here in this corpus and we have words in uppercase, lowercase, just normal sentences. This is our training corpus. Now in the previous demo, we had discussed that the byte pair encoding tokenizer is used by the GPT models. Let's access this tokenizer first using AutoTokenizer.from_pretrained ("gpt2"). That will give us a pre-trained tokenizer used by the GPT-2 model. <br><br>[Video description begins] <em>Line 1 reads: from transformers import AutoTokenizer. Line 3 reads: tokenizer = AutoTokenizer.from_pretrained("gpt2").</em> [Video description ends]</p>
<p>This is our BPE tokenizer. Now the reason I have access this tokenizer is to be able to access the pre-tokenizer that byte pair encoding uses. We'll be implementing the byte pair encoding algorithm that will operate on the output of the pre-tokenizer. The pre-tokenizer is available from the tokenizer for GPT-2. I'm going to use the pre-tokenizer from the GPT-2 tokenizer that we've initialized in order to get all of the words in the training corpus and compute the frequencies of these words. This is the code which does this. On line 3, I initialize a defaultdict in Python of type int. <br><br>[Video description begins] <em>Line 3 reads: word_freqs = defaultdict(int).</em> [Video description ends] That is, the values will be integers. In the defaultdict, the value corresponding to every key will be initialized to 0 by default.</p>
<p>The defaultdict provides a default value for keys that haven't been set in the dictionary yet, and for integers that default value is 0. On line 5, I run a for loop through every example in that corpus, and on line 6, I use the pre-tokenizer to pre-tokenize each string that will give me the words in each example along with the offsets. I extract all of the words on line 8 and I run a nested <br><br>[Video description begins] <em>Line 5 reads: for text in corpus:. Line 6 reads: words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text).</em> [Video description ends]<br><br>for loop through each word and I increment the word_freqs of that word by 1. <br><br>[Video description begins] <em>Line 8 reads: words = [word for word, offset in words_with_offsets]. Line 10 reads: for word in words:. Line 11 reads: word_freqs[word] += 1.</em> [Video description ends]<br><br>This way, each time we encounter a word in the corpus, we increment its count, giving us the frequencies of every word in the corpus. And this is what the result looks like. Observe that the pre-tokenizer adds the G character representing a space for every word that comes after a space in the original text.</p>
<p>We now have the word_freq pairs from our original training corpus. Now, the way the byte pair encoding tokenization algorithm works is that you first compute a base vocabulary and then you add new tokens to this vocabulary till the desired vocabulary size is reached, and you add new tokens by learning merges, which are just rules to merge two elements of the existing vocabulary together into a new one. So, the very first thing that we have to do is set up a base vocabulary, and that's exactly what this code does. To start off with, the base vocabulary will include all of the characters in our training corpus. The characters will be stored in a list called alphabet initialized on line 1.</p>
<p>On line 3, we iterate over the words in the corpus using the keys in the word_freqs dictionary, and for each word, we iterate over the letters in the word on line 5. <br><br>[Video description begins] <em>Line 1 reads: alphabet = []. Line 3 reads: for word in word_freqs.keys():. Line 5 reads: for letter in word:.</em> [Video description ends]<br><br>If the letter in the word has not already been added to the alphabet, we go ahead and append that to the alphabet list on line 8. <br><br>[Video description begins] <em>Line 7 reads: if letter not in alphabet:. Line 8 reads: alphabet.append(letter).</em> [Video description ends] <br><br>We sort the alphabets and we print out to screen so we can see what the characters in the base vocabulary are. <br><br>[Video description begins] <em>Line 10 reads: alphabet.sort(). Line 12 reads: print(alphabet).</em> [Video description ends] <br><br>You can see the punctuations, the special characters, and also the special G character representing the space. These are the tokens in our base vocabulary. Let's take a look at how many tokens we have. <br><br>[Video description begins] <em>Next cell reads: len(alphabet).</em> [Video description ends]<br><br>Length of alphabet gives us that we have 33 tokens to start off with. Every tokenizer uses its own set of special tokens so we need to add those special tokens to be part of the vocabulary as well. <br><br>[Video description begins] <em>Line 1 reads: vocab = [ "&lt;|endoftext|&gt;"] + alphabet.copy(). Line 3 reads: vocab.</em> [Video description ends]<br><br>Now the only special token that the BPE tokenizer uses is this endoftext token which I've now added to my vocabulary.</p>
<p>So, this is what our vocabulary currently has. In order to start the training process for the BPE tokenization algorithm, we need to split each word into individual characters. This will give us a representation of all of the words in the corpus using the base vocabulary of the tokenizer. Remember, the base vocabulary comprises of all individual characters. This code here iterates over every word present in the corpus <br><br>[Video description begins] <em>Line 1 reads: splits = {word:[c for c in word] for word in word_freqs.keys()}. Line 3 reads: splits.</em> [Video description ends] <br><br>and then every character in each word, and thus represents every word as a list of individual characters. If you look at the output, this will become much clearer. The word This is comprised of the characters T, h, i, s, the word is has the space before it, the G, and then the i, s and so on and so forth. So, every word in the corpus is now represented in terms of the base vocabulary, the individual characters. Next, we'll set up a function that computes the frequency of each pair of tokens.</p>
<p>This is a function that will be invoked at each step of the training process of the BPE tokenizer. compute_pair_freqs takes in the splits that we computed in the previous step, and on line 3, we've set up a defaultdict with values that are integers called pair_freqs. <br><br>[Video description begins] <em>Line 1 reads: def compute_pair_freqs(splits):. Line 3 reads: pair_freqs = defaultdict(int).</em> [Video description ends] <br><br>This dictionary will contain pairs of tokens as keys and the value will be the number of times that pair of tokens appear in our training text. Now we know how often each word occurs in the training corpus from our word_freqs dictionary. On line 5, we run a for loop through every key/value pair in the dictionary. The keys are words and the values are frequencies. We look up the split for that particular word on line 6. This split will give us the current tokenized representation <br><br>[Video description begins] <em>Line 5 reads: for word, freq in word_freqs.items():. Line 6 reads: split = splits[word].</em> [Video description ends]<br><br>of the word. The very first time that compute_pair_freqs will be called, the current tokenized representation will comprise of individual characters.</p>
<p>If the length of the split is 1, those are single letter words like a, I, or . or they could be words where the entire word itself is a token. For example, if the tokenized representation of the word the was the word the, then the length of the split would be 1. In that case, there are no pairs of tokens that we can use here to compute frequencies so we simply continue to the next word. Now if there is a word where there are multiple tokens <br><br>[Video description begins] <em>Line 9 reads: if len(split) == 1:. Line 10 reads: continue.</em> [Video description ends]<br><br>representing the word, then on line 12, we run a for loop through every token in the split. The for loop starts at 0 and goes up to the second to last token. On line 13, we access each consecutive pair of tokens at split position i, and split position i+1, and assign these to the tuple pair. And we update the pair_freqs for this pair of tokens. <br><br>[Video description begins] <em>Line 12 reads: for i in range(len(split) - 1):. Line 13 reads: pair = (split [i], split [i + 1]). Line 15 reads: pair_freqs[pair] += freq. Line 17 reads: return pair_freqs.</em> [Video description ends]</p>
<p>The += freq is because we want to increment the frequency of this pair of tokens by as many times as the word occurs in the original corpus. At each step in the training process that this particular function is invoked, it will return the frequencies of every pair of tokens in the training corpus, and we'll use those frequencies to move forward the BPE algorithm. Let's invoke compute_pair_freqs and pass in the current splits information we have. If you look at the pair_freqs, things will make more sense. In the inset picture, you can see that the current splits for [Video description begins] <em>She moves to the next cell. Line 1 reads: pair_freqs = compute_pair_freqs(splits).</em> [Video description ends] each word is that each word is tokenized into its individual characters including the space before the word. This is the input that we pass into compute pair_freqs on line 1. Now let's take a look at the pair_freqs and you can see every pair here is a pair of characters T and h, h and i, i and s.</p>
<p>These are the token pairs from the first word This, whose individual characters you can see in the inset picture T, h, i, s. And for every pair of tokens, we have a corresponding frequency. The combination of uppercase T and h occurs exactly once in our corpus. The combination of i and s occurs twice. The combination of space followed by a occurs 6 times. The combination of a followed by t occurs three times. Thus, for all of the tokens we currently have in our data, we have computed the pair_freqs. The BPE tokenization algorithm to generate the vocabulary of subword tokens will make use of these token pair_freqs to determine how to merge tokens together to get the final vocabulary. Now, the tokens that we'll merge together at every step in the BPE tokenization algorithm will be the pair of tokens that occurs most frequently in our training corpus.</p>
<p>So, we need to write a little bit of code to determine the best pair or the most frequent pair of tokens. So, we've initialized best_pair to the empty string and max_freq to None to start off with. <br><br>[Video description begins] <em>Line 1 reads: best_pair = "". Line 2 reads: max_freq = None.</em> [Video description ends] <br><br>We then run a for loop through every pair and corresponding frequency in the pair_freqs dictionary. If max_freq is None or its less than the frequency of <br><br>[Video description begins] <em>Line 4 reads: for pair, freq in pair_freqs.items():.</em> [Video description ends]<br><br>the current pair of tokens, this becomes the best pair. On lines 6 and 7, we set best_pair to pair and max_freq to the freq of this pair of tokens. This is a very simple for loop to find the best pair and we <br><br>[Video description begins] <em>Line 5 reads: if max_freq is None or max_freq &lt; freq:. Line 6 reads: best_pair = pair. Line 7 reads: max_freq = freq. Line 9 reads: print(best_pair, max_freq).</em> [Video description ends]<br><br>print out the best_pair and the corresponding max_freq out to screen. And in our training output, the token pair with the highest frequency happens to be the space followed by the letter a and this pair occurs a total of 6 times in our corpus.</p>
<p>So, this is the pair of tokens that we merge together in one iteration. We store the merge result in a merges dictionary. We take the space followed by the a. This is the token pair, and the merge result is space followed by a in a single token, and we append this merge token to our vocabulary. <br><br>[Video description begins] <em>Line 1 reads: merges = {("G", "a"): "Ga"}. Line 3 reads: vocab.append("Ga")</em> [Video description ends]<br><br>The vocabulary now comprises of all of the characters that form the original base vocabulary but now we have this additional merge token at the very bottom. This is the process that we'll repeat at every step. We'll compute pair_freqs, find the most frequent pair of tokens, and merge them together, and add a new token to our vocabulary. We'll thus repeatedly merge tokens together till the size of our vocabulary is reached. We'll continue this in the next video.</p></div></div></a></div><div class="section"><a name="section_8"><h2 class="section_title">9. Video: Implementing Byte Pair Encoding Tokenization - II (it_nlpllmdj_01_enus_09)</h2><img src="./Course Transcript_files/image001(8).jpg" alt="" width="300" onclick="(()=&gt;document.getElementById(&#39;lb-image&#39;).src = &#39;https://cdn2.percipio.com/public/b/51b91cdf-38e4-4422-a060-df2bdd131974/image001.jpg&#39;)();"><div class="section_text">In this video, discover how to implement BPE tokenization.</div><div class="section inner"><h3 class="list_title"></h3><ul class="list"><li class="list_item">implement BPE tokenization</li></ul><div class="section_text">[Video description begins] <em>Topic title: Implementing Byte Pair Encoding Tokenization - II. Presented by: Janani Ravi.</em> [Video description ends]
<p>We've set up our utility function to compute pair frequencies, and we've seen how the frequency pair with the highest frequency is merged together to form a single token, so we know one step in the merge process. But this merge process has to be set up as a utility function as well so that this function can be invoked repeatedly in each iteration as we merge tokens together. Here is the function merge_pair. Now here we've already determined what pair of tokens <br><br>[Video description begins] <em>The Implementation_BPETokenizer.ipynb page is open. Below, the page displays multiple lines of code. Line 1 reads: def merge_pair (a, b, splits):.</em> [Video description ends]<br><br>we want to merge, and they're represented by the input arguments a and b. We also pass in the splits dictionary. This is a mapping from each word in our corpus to its current tokenized representations.</p>
<p>Each word will be represented using the tokens in the current vocabulary of the algorithm, and as the vocabulary changes, the split representations need to be updated. That's what merge pair does. On line 2, we have a for loop iterating over each word in the word_freqs dictionaries, and on line 3, we access the current split representation for that word. <br><br>[Video description begins] <em>Line 2 reads: for word in word_freqs:. Line 3 reads: split = splits[word].</em> [Video description ends] <br><br>Now if the length of the split is equal to 1, then this is either a single letter word like a, i, or the period or the word itself is an entire token. So, there's exactly one token to represent the entire word. In such situations, there are no further merges we can perform. We simply continue and move on to the next word. For all other words in the corpus, we'd like to know whether the current pair that we want to merge the tokens in the variables a and b are present in the split representation of that word.</p>
<p>This means we have to check the split representation of each word. On line 9, we initialize i to 0 and run a while loop so long as i is less than the length of the split -1. That is, we'll go up to the last but one token of the split. <br><br>[Video description begins] <em>Line 6 reads: if len(split) == 1:. Line 7 reads: continue. Line 9 reads: i = 0. Line 11 reads: while i &lt; len(split) - 1:.</em> [Video description ends]<br><br>On line 13, we check whether the pair of tokens that we want to merge together is present in the split representation of that word. So, if split at index position i is equal to the first token a, and split at index position i + 1 is equal to the second token b, then the pair that we want to merge together is indeed present in the split representation. On line 15, we merge the terms together so that the new split representation of the word contains the merged pair rather than the individual tokens they contained earlier. If the current pair of tokens don't match the merge pair, we simply move on to the next pair of tokens i += 1 in the else block.</p>
<p>On line 19, whatever updates we've made to the split representation of the <br><br>[Video description begins] <em>Line 13 reads: if split[i] == a and split[i+1] == b:. Line 15 reads: split = split[:i] + [a+b] + split[i + 2 :]. Line 16 reads: else:. Line 17 reads: i += 1.</em> [Video description ends]<br><br>word, we assign to the splits dictionary for that word. What this function does is for each pair of tokens that we want to merge, <br><br>[Video description begins] <em>Line 19 reads: splits[word] = split. Line 21 reads: return splits.</em> [Video description ends]<br><br>it updates the token representations of every word in the corpus using the new merge token rather than the individual tokens that existed before. This will become clearer when you take a look at an example. Let's merge the pair space followed by a and after invoking <br><br>[Video description begins] <em>Line 1 reads: splits = merge_pair("G", "a", splits). Line 3 reads: print (splits["Gabout"]).</em> [Video description ends]<br><br>merge_pair and getting the new splits representation, I'll print out the tokenized representation of the word about preceded by a space. Now the original tokenized representation of about before the merge would have been as follows.</p>
<p>We have the space and then the individual characters of the word about, that would be the tokenized representation. But now that we've merged the space and the a together, this is what the new tokenized representation looks like. You can see it here in the output. Notice that the space and the a have been merged together to form a single token. The remaining characters are separate tokens. Let's try this merge_pair with another word in our training corpus. The Original Tokenized Representation of the word tokenizers preceded by the space would have been as follows. We'd have the space followed by the individual characters of the word tokenizers. Now let's say by computing pair frequencies, we found that the <br><br>[Video description begins] <em>Line 1 reads: splits = merge_pair("e", "n", splits). Line 3 reads: print (splits["Gtokenizers"]).</em> [Video description ends]<br><br>most frequent pair of tokens was e and n, and those are the tokens we are about to merge to form a new token.</p>
<p>After performing the merge_pair operation, the tokenized representations of space followed by tokenizers would be as follows. Notice we have all of the individual characters, but the single characters e and n are merged together to form a single token in the tokenized representation of this word. Let's say we had merged e and n hypothetically because they were the most frequent pair of tokens in our corpus. Let's say we further merge e and r. In the next iteration of the algorithm, let's say that these are the most frequent pair of tokens in our algorithm. Let's take a look at the tokenized representation of space followed by tokenizers, and you can see we have the two merge tokens in there, 'en' and 'er'. In this manner, at every step in the training process of the tokenizer, the most frequent pair of tokens will be merged together to form a single token.</p>
<p>That new merge token will be added to the vocabulary, and all of the tokenize representations of the words in our corpus will be updated so that they are now represented using the merged pair of tokens if needed. Now that we've understood the individual steps in the algorithm, and we have the utility functions for computing pair frequencies and merging tokens together, we can now set up the training process. Let's target a vocab_size of 120. That's what I've initialized on line 1. [Video description begins] <em>Line 1 reads: vocab_size = 120.</em> [Video description ends] This means we'll start off with whatever size our initial vocabulary is. It was only around 33 if I remember correctly. At every step in the training process, we'll take the most frequent pair of tokens, merge them together to form a new token and add that to the vocabulary, and update the tokenized representation of every word in the training data.</p>
<p>On line 3, we run a while loop to continue the training process so long as we not met the target vocabulary size. On line 4, we use compute_pair_freqs to compute the pair_freqs of the current tokenized representations of every word in the training corpus. <br><br>[Video description begins] <em>Line 3 reads: while len(vocab) &lt; vocab_size:. Line 4 reads: pair_freqs = compute_pair_freqs(splits).</em> [Video description ends]<br><br>As we update our tokenized representation of words, we'll have to keep recomputing the pair_freqs. Based on the updated pair frequencies, the code on lines 6 through 10 computes the token pair which has the highest frequency count. This is code that you've seen before. <br><br>[Video description begins] <em>Line 6 reads: best_pair, max_freq = "", None. Line 7 reads: for pair, freq in pair_freqs.items():. Line 8 reads: if max_freq is None or max_freq &lt; freq:. Line 9 reads: best_pair = pair. Line 10 reads: max_freq = freq.</em> [Video description ends] <br><br>Once we've identified the best_pair with the max_freq, on line 12, we call merge_pair to merge the most frequent pair of tokens together and update the tokenize representation of all words in our training data. <br><br>[Video description begins] <em>Line 12 reads: splits = merge_pair(*best_pair, splits).</em> [Video description ends]</p>
<p>The updated splits that we get on line 12, will have every word represented using merged tokens rather than the original token pair. On line 14, we keep track of all of the merges that we perform in the merges dictionary, and on line 15, we add this new merged token to the vocabulary. Thus, we expand our vocabulary with subwords. <br><br>[Video description begins] <em>Line 14 reads: merges[best_pair] = best_pair[0] + best_pair[1]. Line 15 reads: vocab.append(best_pair[0] + best_pair[1]).</em> [Video description ends]<br><br>Once you execute this code, the vocabulary will contain all of the subword tokens that were found from our training data and our vocabulary will also have the size that we wanted to have, that is 120 subword tokens. Now if you take a look at the contents of the merges dictionary, you'll see what merge operations were performed during the training process and here you can see how the frequently <br><br>[Video description begins] <em>Next cell reads: merges.</em> [Video description ends]<br><br>occurring token pairs were merged together.</p>
<p>We started off with individual characters in our base vocabulary, but you can see that we've grown our vocabulary to include many commonly occurring subwords. In fact, there are entire words present in our vocabulary as well. Words such as pair, here, explore, tokenization, are all present in our vocabulary. These were just the merges that were performed. Let's take a look at the actual vocabulary to see what kind of words exist here. <br><br>[Video description begins] <em>Next cell reads: vocab.</em> [Video description ends] <br><br>You can see all of the individual characters are still part of the vocabulary. The individual characters are required here so that we can represent every word in our corpus. And if you scroll down here, you can see the subwords that make up the vocabulary as well. Each time we perform a merge, a new subword token is added to the vocabulary.</p>
<p>Now that we have the subword tokens in our vocabulary, we can put everything together and write a function to tokenize any input text. Here is my tokenize function that takes in some text as an input argument. On line 3, we pre_tokenize the text to get the pre_tokenize output and on line 4, we get the list of words in the pre_tokenize output.<br><br>[Video description begins] <em>Line 1 reads: def tokenize(text):. Line 3 reads: pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text). Line 4 reads: pre_tokenized_text = [word for word, offset in pre_tokenize_result].</em> [Video description ends] <br><br>On line 6, we split every word in the text to be represented as a list of characters. <br><br>[Video description begins] <em>Line 6 reads: splits [[l for l in word] for word in pre_tokenized_text].</em> [Video description ends]<br><br>So, if the original text that you want to tokenize is Hello, good morning, then this text will be represented as a list of lists, where the inner list just contains the characters from the individual words in the original text. You can see an example here to understand what the code in line 6 does. Now we have all of the merges that can be performed on this representation of the input text in the merges dictionary.</p>
<p>We iterate over every key-value pair in the mergers dictionary on line 9, and then we have another nested for loop on line 11, iterating over each tokenized representation of the individual words. Currently, the splits are represented as individual characters, we'll now merge those together to form tokens. <br><br>[Video description begins] <em>Line 9 reads: for pair, merge in merges.items():. Line 11 reads: for idx, split in enumerate(splits):.</em> [Video description ends] <br><br>The code on lines 13 through 20, is code that you've seen before, at least code similar to it. We check to see whether a merge pair is present in the representation of each word, and if yes, on line 18, we merge those two tokens together to form a single subword token, and on line 22, we update the tokenized representation of that word. <br><br>[Video description begins] <em>Line 18 reads: split = split[:i] + [merge] + split [i + 2 :].</em> [Video description ends]</p>
<p>In this way, starting from single-character representations <br><br>[Video description begins] <em>Line 22 reads: splits[idx] = split.</em> [Video description ends] <br><br>of each word in the original text, we merge the single characters together and finally have each word represented using subword tokens present in our vocabulary, and we take help of the merges dictionary for this. And we return the sum of splits, that simply flattens out the list of lists that we have in the splits variable and returns a single list with subword tokens representing the entire text. <br><br>[Video description begins] <em>Line 24 reads: return sum (splits, []).</em> [Video description ends] <br><br>Let's try using our BPE-tokenized vocabulary to represent some arbitrary text.</p>
<p>"This is an implementation of the Byte-pair Encoding tokenizer." And the result you see is a tokenize representation of this text. <br><br>[Video description begins] <em>Next cell reads: tokenize("This is an implementation of the Byte-pair Encoding tokenizer.").</em> [Video description ends] <br><br>The tokenizer was trained on our limited corpus and that's why you see many of the individual characters are still used to represent words. You can see that the word implementation is almost entirely represented using the individual characters i, m, p, le, m, and so on. But if you scroll down below, you'll see that the word 'Byte' was present in our corpus and that's why that's a complete token, as was the word 'Encoding' that was present in our corpus, and it forms a complete token here in the tokenized result.</p></div></div></a></div><div class="section"><a name="section_9"><h2 class="section_title">10. Video: Implementing Wordpiece Tokenization - I (it_nlpllmdj_01_enus_10)</h2><img src="./Course Transcript_files/image001(9).jpg" alt="" width="300" onclick="(()=&gt;document.getElementById(&#39;lb-image&#39;).src = &#39;https://cdn2.percipio.com/public/b/3b106ee2-b360-4fb3-bc4c-682d889f4a2b/image001.jpg&#39;)();"><div class="section_text">Learn how to set up a WordPiece tokenizer.</div><div class="section inner"><h3 class="list_title"></h3><ul class="list"><li class="list_item">set up a WordPiece tokenizer</li></ul><div class="section_text">[Video description begins] <em>Topic title: Implementing Wordpiece Tokenization -I. Presented by: Janani Ravi.</em> [Video description ends]
<p>In this demo, we'll work through an actual implementation of the WordPiece Tokenization algorithm. WordPiece is the tokenization algorithm Google developed to pre-train the BERT model. It has since then been reused in quite a few transformer models that are based on BERT, such as DistilBERT, MobileBERT, Funnel Transformers, and others. Now it's very similar to the byte pair encoding tokenization in terms of the training, but the actual tokenization is a little different.</p>
<p>The code that we'll discuss here is originally from the Hugging Face documentation. Now the Hugging Face documentation warns us that since Google never open-sourced its implementation of the training algorithm for WordPiece, this implementation that we are going to see here is an educated guess based on whatever literature has been published on WordPiece. You'll see that the basic structure of the WordPiece algorithm is very similar to the byte pair encoding algorithm that we studied earlier. We'll start with the base vocabulary of characters and learn merge rules, which we'll then use to merge together tokens to form subword tokens.</p>
<p>The fundamental structure of the algorithm will be the same as byte pair encoding tokenization, but what will be different is in how this particular algorithm chooses the pair of tokens to be merged. Instead of selecting the most frequent pair and then merging those pair of tokens together to form a new token, WordPiece computes a score for each pair of tokens and then chooses the pair of tokens with the highest score. The score for every token pair is computed using the pair. We compute the frequency of the pair of tokens and divide that by the frequency of the first element in the token multiplied by the frequency of the second element token.</p>
<p>So, remember this formula? We'll be implementing this in code. Let's get started with the WordPiece tokenization algorithm. Once again, we need a corpus of training data, and we'll use the same corpus as before. These are the same examples that we had in the previous demo for byte pair encoding tokenization. <br><br>[Video description begins] <em>A page titled Implementation_WordpieceTokenizer.ipynb is open. The first cell displays multiple lines of code. Line 1 reads: corpus = [. Line 2 reads: "This is a Natural Language Processing course.",. The rest of the lines display a similar pattern of code.</em> [Video description ends]<br><br>We'll implement WordPiece tokenization by starting off with the pre-tokenized output and we'll need to access the pre-tokenizer from the bert-base-cased model. I use AutoTokenizer.from_pretrained to load in the tokenizer <br><br>[Video description begins] <em>The following line of code is highlighted in the next cell: tokenizer = AutoTokenizer.from_pretrained("bert-base-cased").</em> [Video description ends]<br><br>for the bert-base-cased model.</p>
<p>Again, we won't be using this tokenizer directly, we need the pre-tokenized output. The next step is again a step that we've seen before in our earlier demo. This is where we initialize a dictionary called word_freqs with every word from our training data map to how often that word occurs in the training corpus. <br><br>[Video description begins] <em>Line 1 reads: from collections import defaultdict. Line 3 reads: word_freqs = defaultdict(int).</em> [Video description ends]<br><br>This will be the word_freqs. We use the defaultdict as before. All of the words will be mapped to a frequency of 0 to start off with. We iterate over each text in the corpus on line 5 and we pre-tokenize that text and get the words with offsets on line 7. <br><br>[Video description begins] <em>Line 5 reads: for text in corpus:. Line 7 reads: words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text).</em> [Video description ends]<br><br>We get a list of all of the words from the output of the pre-tokenizer on line 9, and for each word on line 11, we increment the frequency count each time we encounter the word.<br><br>[Video description begins] <em>Line 9 reads: words = [word for word, offset in words_with_offsets]. Line 11 reads: for word in words:. Line 12 reads: word_freqs[word] += 1.</em> [Video description ends]<br><br>This will give us our word_freqs dictionary, and this is the same dictionary that we saw in the previous demo.</p>
<p>You can see that the word tokenizers appears twice, the word will appears thrice, and so on. Observe that the pre-tokenized output does not include the space character unlike in byte pair encoding tokenization. The next step is to create a base vocabulary which will comprise of all of the characters in the input text. This is the base vocabulary that we'll start off with that we'll then merge together to form the subword tokens of our final vocabulary. I've initialized an alphabet list on line 1 and I iterate</p>
<p>over every word in our corpus on line 3. Now the way we add the characters to our base vocabulary is <br><br>[Video description begins] <em>Line 1 reads: alphabet = []. Line 3 reads: for word in word_freqs.keys():.</em> [Video description ends]<br><br>going to be a little different in WordPiece as compared to BPE. The first token in each word is appended to our vocabulary that is alphabet as is. This is the code on lines 5 and 6. <br><br>[Video description begins] <em>Line 5 reads: if word[0] not in alphabet:. Line 6 reads: alphabet.append(word[0]).</em> [Video description ends]<br><br>If word of 0 is not an alphabet we just append it to the alphabet. The remaining subword tokens from the same word will be prepended by the special characters the ##. So, on line 8, we run a for loop through all of the other characters in the word starting from the second position, that is index position 1. And on line 9, we check whether that character or letter prepended by the ## is present in the alphabet if not we append it to the alphabet on line 10. <br><br>[Video description begins] <em>Line 8 reads: for letter in word[1:]:. Line 9 reads: if f"##{letter}" not in alphabet:. Line 10 reads: alphabet.append(f"##{letter}").</em> [Video description ends]</p>
<p>This ## prefix is used by BERT to identify subwords, that is all sub-tokens of a word other than the first sub-token. The first token does not have this ## prefix. Let's go ahead and take a look at our alphabet. All of the characters you see with the ## prefix, these are sub-tokens that are not the first token. And here all of the characters that you see without the ## prefix are the first sub-token of the words in our corpus. The punctuation characters such as the comma and the period are always considered to be separate words and that's why they don't have the ## prefix. Let's take a look at the size of the base vocabulary length of alphabet will give us this information and you can see that it's rather small. It's only 39.</p>
<p>Every tokenizer has some special tokens that it uses to represents different kinds of information, and these are the special tokens used by the WordPiece tokenizer and the BERT model, <br><br>[Video description begins] <em>Line 1 reads: vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy().</em> [Video description ends]<br><br>the PAD token, the UNK token, CLS, SEP, and the MASK token. You can see this tokenizer has many more special tokens. We add these to our vocabulary and this is what our vocabulary looks like. We have all of the special tokens and the characters extracted from our training corpus. Next, we initialize the splits variable, which is a dictionary containing every word in our natural language corpus as the key, and the values will be the tokenized representation of each word. <br><br>[Video description begins] <em>She highlights the following lines of code. Line 1 reads: splits = {. Line 2 reads: word:[c if i == 0 else f"##{c}" for i, c in enumerate (word)]. Line 3 reads: for word in word_freqs.keys(). Line 4 reads: }.</em> [Video description ends]<br><br>This tokenized representation uses the current tokens that we have in the base vocabulary. Every character in the word is a separate token. Once again, the representation here is a little different.</p>
<p>Let me show you how you will actually interpret this code. On line 3, we iterate over every word in our training corpus. This word is the key in the splits dictionary. You see word before the colon on line 2. The value corresponding to each word will be the tokenized representation. The tokens will be the individual characters that make up the word. If it's the first character, then the token is the character itself. For all subsequent characters, we have the ## prefix. Every sub-token in a word other than the first token will have that ## prefix, and if you look at splits, you'll understand how each word is tokenized. Let's take the example of the first word This T, which is the first sub-token in the word, is just standalone. The remaining sub-tokens all have the ## prefix. h, i, and s all have the ## prefix.</p>
<p>Let's look at another word and its tokenized representation. Somewhere below we have the word course. The first token, c does not have the special prefix, but o, u, r, s, and e has the special ## prefix. The next step is to compute the scores for each token pair. It is this score that tokenization algorithm uses to determine what pair of tokens should be merged together to form a new subword token. Let me remind you once again how the scores are computed for each token pair. The score is the frequency of the pair of tokens divided by the frequency of the first element multiplied by the frequency of the second element. So, the compute pair scores function has to keep track of two kinds of frequencies, the frequency of a merged pair of tokens and the frequency of the individual tokens before the merge is actually performed. By dividing the frequency of the pair by the product of the frequencies of each of its path, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary.</p>
<p>If the individual parts tend to occur more commonly in the vocabulary, the pair will tend to not be merged. Now let's turn our attention to the code. compute_pair_scores takes as an input argument, splits the dictionary mapping every word to its corresponding tokenized representation with the current vocabulary. On lines 2 and 3, <br><br>[Video description begins] <em>Line 1 reads: def compute_pair_scores(splits):.</em> [Video description ends] <br><br>we initialize the dictionaries, keeping track of the two sets of frequencies that we discussed. <br><br>[Video description begins] <em>Line 2 reads: letter_freqs = defaultdict(int). Line 3 reads: pair_freqs = defaultdict(int).</em> [Video description ends] <br><br>letter_freqs have the frequencies of the tokens before the merge has been performed. pair_freqs contain the frequencies of pairs of tokens. On line 5, we run a for loop to iterate over words and the corresponding frequencies from our natural language corpus. On line 6, I access the split for each word that is the tokenized representation of each word. On lines 9 through 11, we check to see whether the length of the split is 1. <br><br>[Video description begins] <em>Line 5 reads: for word, freq in word_freqs.items():. Line 6 reads: split = splits[word].</em> [Video description ends]<br><br>This will be true if we have single-letter words like a or i or if the entire word is a subword token. <br><br>[Video description begins] <em>Line 9 reads: if len(split) == 1:. Line 10 reads: letter_freqs[split[0]] += freq. Line 11 reads: continue.</em> [Video description ends]<br><br>In that case, we simply increment the letter_freqs by the frequency of that word and continue to the next iteration. The for loop on line 14, iterates over all of the tokens for the current word up to the last but one token, and we create token pairs on line 15. <br><br>[Video description begins] <em>Line 14 reads: for i in range(len(split) - 1):. Line 15 reads: pair = (split [i], split [i + 1]). Line 15 reads: pair_freqs[pair] += freq. Line 17 reads: return pair_freqs.</em> [Video description ends]<br><br>Pairs are just contiguous tokens, tokens that come one after the other. On lines 17 and 18, we populate the frequencies for the individual tokens as well as for the pair. <br><br>[Video description begins] <em>Line 17 reads: letter_freqs[split[i]] += freq. Line 18 reads: pair_freqs[pair] += freq.</em> [Video description ends]</p>
<p>The last individual token in each split that is tokenized representation of a word also has to be added. This we do on line 21. This for loop on lines 14 through 21, will populate <br><br>[Video description begins] <em>Line 21 reads: letter_freqs[split[-1]] += freq.</em> [Video description ends]<br><br>the letter_freqs and the pair_freqs. This will give us the frequencies of the individual elements as well as the pair and this is what we'll use to compute the score here below on lines 23 through 26. On line 25, I run a for loop for every pair in the pair_freqs, <br><br>[Video description begins] <em>Line 23 reads: Scores = {. Line 24 reads: pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]]). Line 25 reads: for pair, freq in pair_freqs.items(). Line 26 reads: }.</em> [Video description ends] <br><br>and for each pair, I compute the corresponding score on line 24. The score has the pair frequency in the numerator and in the denominator, we find the frequency of the first element and multiply that by the frequency of the second element in the pair and we return the scores for all pairs. Let's take a look at the output of this function that we've just defined. Compute pair_scores, pass in splits, and let's see what the pair_scores look like. <br><br>[Video description begins] <em>Line 1 reads: pair_scores = compute_pair_scores(splits).</em> [Video description ends]<br><br>Observe that for every pair of tokens in the base vocabulary, we now have a corresponding score. So, you can see that the score for the pair T and h is 1, which is higher than the score for the pair h and i, which is just 0.09.</p>
<p>The score for the pair i and s is even lower at 0.01. So, remember the pairs with the higher scores will tend to be merged first. Figuring out which pair has the best score is something that we've done before. It's a simple for loop to keep track of the best_pair and the max_score, and the best_pair happens to be T and h with the score of 1. <br><br>[Video description begins] <em>Line 1 reads: best_pair = "". Line 2 reads: max_score = None. Line 4 reads: for pair, score in pair_scores.items():. Line 5 reads: if max_score is None or max_score&lt; score:. Line 6 reads: best_pair = pair. Line 7 reads: max_score = score.</em> [Video description ends]<br><br>This is the pair of tokens that will be merged first. Let's see how the merge operation will be performed. The vocabulary will be appended with this pair of tokens so that <br><br>[Video description begins] <em>Line 1 reads: vocab.append("Th").</em> [Video description ends]<br><br>we have a new vocabulary with this additional subword token Th added to the very bottom.</p></div></div></a></div><div class="section"><a name="section_10"><h2 class="section_title">11. Video: Implementing Wordpiece Tokenization - II (it_nlpllmdj_01_enus_11)</h2><img src="./Course Transcript_files/image001(10).jpg" alt="" width="300" onclick="(()=&gt;document.getElementById(&#39;lb-image&#39;).src = &#39;https://cdn2.percipio.com/public/b/3d186b1b-5f3d-4925-98ba-30a11a44dd7f/image001.jpg&#39;)();"><div class="section_text">Discover how to implement WordPiece tokenization.</div><div class="section inner"><h3 class="list_title"></h3><ul class="list"><li class="list_item">implement WordPiece tokenization</li></ul><div class="section_text">[Video description begins] <em>Topic title: Implementing Wordpiece Tokenization -II. Presented by: Janani Ravi.</em> [Video description ends]
<p>Now that we have the function to compute the pair scores at every step in the training of our tokenization algorithm, the next step is to set up a function to merge pairs of tokens together. This function is similar to the one that we've used for the BPE tokenization algorithm with just a few differences, let's take a look.</p>
<p>The function is called merge_pair. It takes in a and b, the pair of tokens that needs to be merged, and it takes in the splits dictionary which contains the tokenized representation of each word. We run a for loop through each word in the word_freqs and then we look up the tokenized representation of that word on line 3. If the length of the split is equal to 1, those are <br><br>[Video description begins] <em>The Implementation_WordpieceTokenizer.ipynb page is open. Below, the page displays multiple lines of code. Line 1 reads: def merge_pair (a, b, splits):. Line 2 reads: for word in word_freqs:. Line 3 reads: split = splits[word].</em> [Video description ends]<br><br>single-letter words or words where the entire word is the token itself. We perform this check on lines 6 and 7, and for single token words, we just continue to the next word to be processed.</p>
<p>On line 11, we run a while loop iterating over the tokens for a particular words and we check to see whether the tokens contain the <br><br>[Video description begins] <em>Line 6 reads: if len(split) == 1:. Line 7 reads: continue. Line 9 reads: i = 0. Line 11 reads: while i &lt; len(split) - 1:.</em> [Video description ends]<br><br>pair of tokens that we are about to merge. This check is on line 13. We check whether the current token is equal to a and the next token is equal to b. If this is found to be true, we merge the terms on line 15. <br><br>[Video description begins] <em>Line 13 reads: if split[i] == a and split[i+1] == b:.</em> [Video description ends]<br><br>[Video description begins] <em>Line 15 reads: merge = a+b[2:] if b.startswith("##") elsea+b. Line 18 reads: split = split[:i] + [merge] + split[i + 2:].</em> [Video description ends]<br><br>Now we have to take care of the ## prefix. If the second token, b starts with the ##, then do not include the ## as a part of the merge token. Otherwise, we include the ##.</p>
<p>On line 18, we change the tokenized representation of the word to include the merge token rather than the original tokens. So, we get rid of the original terms and include the merged term here. On line 22, now that we have the new tokenized representation for the word, we update that in the splits dictionary and return the splits dictionary on line 24. <br><br>[Video description begins] <em>Line 22 reads: splits[word] = split. Line 24 reads: return splits.</em> [Video description ends] <br><br>Let's confirm that this merge_pair function works as we expect it to. I'm going to merge the token pairs T and h together, h is a subword token so it has the ## prefix, and then print out the tokenized representation of the word "This". And you can see that the tokenized representation of this <br><br>[Video description begins] <em>Line 1 reads: splits = merge_pair("T", "##h", splits). Line 3 reads: splits["This"].</em> [Video description ends] <br><br>includes the merged pair T and ' are merged together and the ## prefix has been removed from the merge token.</p>
<p>Let's try this once again. I'm going to merge the tokens t and o together and then take a look at how the word tokens is represented using this merge_pair and you can see that the terms 'to' form a single subword token <br><br>[Video description begins] <em>Line 1 reads: splits = merge_pair("t", "##o", splits). Line 3 reads: splits["tokens"].</em> [Video description ends]<br><br>and then we have the individual characters as before. All of the words that contain the pair 'to' will have their tokenized representations updated. For example, if you look at the tokenized representation for "tokenizers", you can see that the merged pair where 'to' is present in this representation as well. <br><br>[Video description begins] <em>Next cell reads: splits["tokenizers"].</em> [Video description ends]</p>
<p>We are now ready to train the WordPiece tokenization algorithm on the corpus of training data that we have. Now, we'll aim for a vocab_size of 120 subword tokens. So, long as the length of the vocabulary is less than this vocab_size, that's the while loop on line 3, we'll run the while loop. For the current tokenized representation of each word in our corpus, we'll compute the pair_scores on line 5. Then on lines 7 through 11, we figure out what pair has the best score. <br><br>[Video description begins] <em>Line 1 reads: vocab_size = 120. Line 3 reads: while len(vocab) &lt; vocab_size:. Line 5 reads: scores = compute_pair_scores(splits).</em> [Video description ends]</p>
<p>Once we find the best_pair on line 13, we go ahead and merge <br><br>[Video description begins] <em>Line 7 reads: best_pair, max_score = "", None. Line 8 reads: for pair, score in pair_scoress.items():. Line 9 reads: if max_score is None or max_score &lt; score:. Line 10 reads: best_pair = pair. Line 11 reads: max_score = score.</em> [Video description ends] <br><br>the tokens of the best_pair, and we get a new splits dictionary with new tokenized representations using the merge_pair for every word in our corpus. On lines 15 through 18, we compute what the new merge token is. That's just the two elements of the best_pair merged together. We just have to account for the ## prefix, and then we <br><br>[Video description begins] <em>Line 13 reads: splits = merge_pair(*best_pair, splits). Line 15 reads: new_token = (. Line 16 reads: best_pair[0] + best_pair[1][2:] if best_pair[1]. startswith("##"). Line 17 reads: else best_pair[0] + best_pair[1].</em> [Video description ends]<br><br>append this new_token to the vocabulary. So, the vocabulary constantly updates with the new subword tokens that we generate.</p>
<p>Once we run this training, let's take a look at what the final <br><br>[Video description begins] <em>Line 20 reads: vocab.append (new_token).</em> [Video description ends]<br><br>vocabulary for our tokenizer is like. <br><br>[Video description begins] <em>Next cell reads: vocab.</em> [Video description ends] <br><br>You can see all of the special tokens up top, and you can see the tokens for the individual characters and then we can see the subword tokens that we have created using merges. Notice that the word 'Natural' is a subword by itself. That's the entire word. The word 'train' is also a subword token in our vocabulary.</p>
<p>Now that we have a trained tokenizer, let's write a utility function that will allow us to get the tokenized representation for any word. This is the encode_word function. On line 2, the tokens list will contain the tokenized representation at the end of processing. On line 4, we run a while loop to break the input word into subword tokens. <br><br>[Video description begins] <em>Line 1 reads: def encode_word(word):. Line 3 reads: while len(word) &gt; 0:.</em> [Video description ends]<br><br>We continue so long as the length of the word is greater than 0. On line 5, we initialize i to the current length of the word. This will change in the process of the while loop.</p>
<p>On lines 9 and 10, we have a nested while loop which tries to find the longest part of the word that is present in our subword token vocabulary, starting with the entire word itself. <br><br>[Video description begins] <em>Line 5 reads: i = len(word). Line 9 reads: while i &gt; 0 and word[:i] not in vocab:. Line 10 reads: i -= 1.</em> [Video description ends]<br><br>If the whole word is present in our subword token vocabulary, then the entire word can be represented using a single token. If the whole word is not found in the vocabulary, we'll look at the part of the word, we drop the last character, and look at the rest of the word, all but the last character and we continue this in the longest part of the word starting from the beginning of the word is found in our subword token vocabulary.</p>
<p>The code will come to line 14, once we find the longest part of the word that is present in our vocabulary. Now if i == 0 at this point, it means that no part of the word was found in our vocabulary and the entire word is essentially unknown to our tokenizer so we return the UNK token. <br><br>[Video description begins] <em>Line 14 reads: if i == 0:. Line 15 reads: return ["[UNK]"].</em> [Video description ends]<br><br>The execution comes to line 19, if some part of the word is present in our subword token vocabulary and we append that part of the word found in the vocabulary to the tokens list. And on line 20, we consider the rest of the word, which we haven't yet processed.</p>
<p>So, word will now only contain a part of the original word that was not found in the vocabulary. On line 22, we check to see whether the length of the word is greater than 0. If yes, that is, there is a portion of the word that we still need to process. If it's going to be part of any tokens, it will have to have the ## prefix. So, we go ahead and add that. [Video description begins] <em>Line 19 reads: tokens.append(word[:i]). Line 20 reads: word = word[i:]. Line 22 reads: if len(word) &gt; 0:. Line 23 reads: word = f"##{word}".</em> [Video description ends] This while loop continues till the entire word is processed and we return the tokens for the input word on line 25. Alright, let's try this out.</p>
<p>Let's encode two words, "Natural" and "Notaral". Now the word "Natural" is present in our vocabulary in its entirety, <br><br>[Video description begins] <em>Line 1 reads: print(encode_word("Natural")). Line 2 displays a similar pattern of code.</em> [Video description ends] <br><br>whereas the word "Notaral" is not present in our vocabulary. Let's see the tokenized representation of these words. You can see the word 'Natural' is a token by itself, whereas the word 'Notaral' has been broken into individual subword tokens. ta, al, are all subwords present in our vocabulary. Let's try this again with another word, the word "tokenizing". This exact word is not present in our corpus, but some variation of this word is.</p>
<p>So, you can see the subwords that have been created to represent this original word. tok, niz, g, these are the subwords. Let's try the word "exploring". The word explor is definitely present in our vocabulary. Here is a tokenized representation of the word "exploring". Let's try another word here the word "Label". The individual characters are definitely present in our vocabulary and here is the tokenized representation. L and a is a single token, the remaining characters are all single-character tokens. We've got the helper function to get the tokenized representation of a single word.</p>
<p>Let's now set up a tokenize function to tokenize a complete example, a complete sentence. On line 2, we first pre_tokenize the text so we get the individual words. On line 4, we set up a list of the words from the output of the pre_tokenizer. <br><br>[Video description begins] <em>She highlights the following line: pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)</em> [Video description ends]<br><br>We invoke encode_word on each word in the pre_tokenized_text and this will be a list of lists where we have the tokenized representation of each word. sum (encoded_words, []) will convert this to a single flattened list.<br><br>[Video description begins] <em>Line 4 reads: pre_tokenized_text = [word for word, offset in pre_tokenize_result]. Line 6 reads: encoded_words = [encode_word(word) for word in pre_tokenized_text]. Line 8 reads: return sum (encoded_words, []).</em> [Video description ends]</p>
<p>Let's see this in action. Let's try with the first sentence. "tokenizing language is fun" and here is the tokenize representation. <br><br>[Video description begins] <em>Next cell reads: tokenize("tokenizing language is fun").</em> [Video description ends] <br><br>Observe that we have an UNK token at the very end. That's for the word fun. The individual characters of the word and the word itself does not occur in our vocabulary. Our training corpus was so tiny that it did not include the letters fun and that's why fun is an UNK token. But when you're working in the real world, your tokenizer is going to be trained on a huge corpus of data.</p>
<p>Let's use the original BertTokenizer. I'm going to read it in from "bert-based-uncased" <br><br>[Video description begins] <em>Next cell displays various lines. Line 3 reads: tokenizer = BertTokenizer.from_pretrained("bert-base-uncased").</em> [Video description ends]<br><br>and then I'm going to use that to tokenize "tokenizing language is fun". This BertTokenizer has been pre-trained on a very large corpus where, of course, the word 'fun' occurs, and here you can see that 'fun' is not an unknown token. The tokens themselves are different from the tokens that we got with our limited vocabulary.</p></div></div></a></div><div class="section"><a name="section_11"><h2 class="section_title">12. Video: Building and Training a BPE Tokenizer (it_nlpllmdj_01_enus_12)</h2><img src="./Course Transcript_files/image001(11).jpg" alt="" width="300" onclick="(()=&gt;document.getElementById(&#39;lb-image&#39;).src = &#39;https://cdn2.percipio.com/public/b/35dae69a-28a8-4ca7-bd4e-7a2989c6de28/image001.jpg&#39;)();"><div class="section_text">In this video, find out how to train a BPE tokenizer.</div><div class="section inner"><h3 class="list_title"></h3><ul class="list"><li class="list_item">train a BPE tokenizer</li></ul><div class="section_text">[Video description begins] <em>Topic title: Building and Training a BPE Tokenizer. Presented by: Janani Ravi.</em> [Video description ends]
<p>Now that we've seen the intricate details of training a tokenization algorithm, in this demo, I'll show you how you can build up a tokenizer block by block using Hugging Face libraries. We won't be implementing the tokenizer or training the tokenizer, but we'll bring together the different functional blocks that make up a tokenizer. Tokenization comprises of several steps and here in this demo, we'll see how you can configure and build up these steps to set up your tokenizer.</p>
<p>Here are the broad functional steps in tokenization. We start off with normalization, that is text cleanup, removing spaces, accents, Unicode normalization. The next step is pre-tokenization, splitting the input into words. Then we run the input through our model using the pre-tokenized words to produce a sequence of tokens. Once we have the tokens, we perform some post-processing. This may involve adding special tokens of the tokenizer, generating some attention mask, and some other details.</p>
<p>In this demo, we'll follow these basic steps to build up a byte pair encoding tokenizer. Now the transformers library and the datasets library from Hugging Face are already available on Colab. However, just in case, you want some updated version, you can always pip install them if you want to, as I've done here. We'll set up a corpus of data on which we'll apply the tokenizer <br><br>[Video description begins] <em>A page titled TokenizersFromScratch_BPE.ipynb is open. Below, a code cell displays the following code: pip install transformers datasets.</em> [Video description ends]<br><br>that we built up block by block, and in order to access that data corpus, I'm going to use the Hugging Face datasets.</p>
<p>In order to access the datasets, you'll need to import huggingface_hub and then call the list_datasets function and this will <br><br>[Video description begins] <em>Line 1 reads: import huggingface_hub. Line 3 reads: list(huggingface_hub.list_datasets()).</em> [Video description ends] <br><br>show you the entire list of datasets available. As you can see, there are a large number of datasets and it's hard to parse information from here. I recommend that you look at the datasets that you're interested in by heading over to the Hugging Face UI and choosing the dataset from there and then loading it into your program. <br><br>[Video description begins] <em>Line 1 reads: len(list(huggingface_hub.list_datasets())).</em> [Video description ends]<br><br>At the time of this recording, the number of datasets available in Hugging Face was a 103000 approximately, and this is only going to grow.</p>
<p>Once you've figured out what dataset you want to use using the Hugging Face UI, you can simply load that dataset into your program using the load_dataset function. <br><br>[Video description begins] <em>Line 1 reads: from datasets import load_dataset. Line 4 reads: ag_news_dataset = load_dataset("ag_news", split = "train[:15000]").</em> [Video description ends]<br><br>This is the corpus that we'll use to train our tokenizer, once we've set up the functional blocks of the tokenizer, I'll use only a small subset of documents from this corpus so that the training process runs fast. On line 4, I invoke load_dataset, I specify the name "ag_news", I'm only looking for the training data, and I'm only interested in the first 15000 records.</p>
<p>The original dataset is much larger. I'm not interested in the entire data. This will load the dataset into my program as a Dataset object. You can see the features of this dataset, there is a text column and a label column, and the total number of rows is equal to 15000. That's what I had specified. Now this is a news_dataset. Let's take a look at the text for one of the rows in this dataset. I've picked row with index 1 at random. <br><br>[Video description begins] <em>Next cell reads: ag_news_dataset['text'][1].</em> [Video description ends]<br><br>And here in the result, you can see that this text is some kind of news article, and all of the text will be news articles.</p>
<p>Now we are only interested in text_samples and not the label. This is because we are going to be using the text for tokenization. We are not really training any model. We are not training a classification model. I'll extract all of the text_samples and here are the first 5. We now have a list of text_samples that we can work with. <br><br>[Video description begins] <em>Line 1 reads: text_samples = [example['text'] for example inag_news_dataset]. Line 3 reads: text_samples[:5]</em> [Video description ends] <br><br>All of the building blocks needed to build our tokenizer is available from the Hugging Face library. Now the class at the core of it all is the Tokenizer class, <br><br>[Video description begins] <em>Line 1 reads: from tokenizers import (. Line 2 reads: decoders,. Line 3 reads: models,. Line 4 reads: normalizers,. Line 5 reads: pre_tokenizers,. Line 6 reads: processors,. Line 7 reads: trainers,. Line 8 reads: Tokenizer,.</em> [Video description ends]<br><br>which you can see imported on line 8. The blocks that provide the other functionality make up the rest of the imports, and they are all grouped into sub-modules. The normalizers sub-module on line 4, that's what contains the classes that we'll use to normalize our data.</p>
<p>We have pre_tokenizers imported on line 5. Models contains the various types of tokenization models that you can use, whether it's BPE, WordPiece, or Unigram. Trainers contain all of the different types of trainer classes that you can use to train your model on a corpus. There are different trainers for different models. Processors contains the various types of post-processing you can perform on your data, and decoders allow you to decode the output of tokenization.</p>
<p>On line 11, I've instantiated the kind of tokenizer that I'm first going to build and train. <br><br>[Video description begins] <em>Line 11 reads: tokenizer = Tokenizer(models.BPE()).</em> [Video description ends]<br><br>This is a BPE or byte pair encoding tokenizer. Go ahead and instantiate this. Now if you remember, the BPE tokenizer is what GPT-2 uses. Now, GPT-2 does not use a normalizer, so we won't be initializing a normalizer here and we'll go directly to pre-tokenization. The ByteLevel pre_tokenizer operates on a stream of bytes, meaning it treats each byte of the input text as a separate unit. This does not work on a character or word level. <br><br>[Video description begins] <em>Line 1 reads: tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space = False). Line 3 reads: tokenizer.pre_tokenizer.pre_tokenize_str("We're checking pre-tokenization step.").</em> [Video description ends]</p>
<p>When you work at the ByteLevel, the base vocabulary has a small size, only 256, but every character that you can think of will still be included and will not be converted to the unknown token. I've set add_prefix_space to False, indicating that we do not want to add a space at the beginning of a sentence. Now that we've instantiated this pre_tokenizer to our tokenizer, let's go ahead and pre_tokenize the string and see what the result looks like. This is the output of the ByteLevel pre-tokenization. Notice the space before the word 'checking' and 'pre', represented using the strange G character. There is no space before 'We' at the beginning of the sentence.</p>
<p>Since GPT-2 does not use a normalizer, we haven't included a normalizer here in this tokenizer. <br><br>[Video description begins] <em>Next cell reads: tokenizer.pre_tokenizer.pre_tokenize_str("The pièce de rèsistance was the chef's Special Dessert.").</em> [Video description ends] <br><br>So, when we pre_tokenize the string, all of the accents and other special characters will be present, but those characters are represented at the byte level. In order to train our tokenizer on the corpus of data that we have from the ag_news_dataset, you need to instantiate a trainer.</p>
<p>Now every tokenizer has its own trainer so notice that I've instantiated the BpeTrainer here on line 1. I've specified the size of the vocabulary here, 20000 subword tokens <br><br>[Video description begins] <em>Line 1 reads: trainer = trainers.BpeTrainer(vocab_size = 20000, special_tokens = [ "&lt;|endoftext|&gt;"]).</em> [Video description ends]<br><br>and the only special_tokens I include is this endoftext token, which is what the GPT-2 uses. And I kick start the training process by calling tokenizer.train_from_iterator <br><br>[Video description begins] <em>Line 3 reads: tokenizer.train_from_iterator(text_samples, trainer = trainer).</em> [Video description ends] <br><br>and I pass in the text_samples and the trainer. This will train our tokenizer on our training corpus.</p>
<p>Training should run through quickly and now that we have a train tokenizer, let's get the tokenized representation of the sentence. "We're checking pre-tokenization step." Let's look at the encoding and you can see Encoding here is an <br><br>[Video description begins] <em>Line 1 reads: encoding = tokenizer.encode("We're checking pre-tokenization step.").</em> [Video description ends] <br><br>object that contains a total of 11 tokens and a number of different attributes, and these are attributes that we can look up for each encoding. <br><br>[Video description begins] <em>Line 1 reads: print("Encoding_ids :", encoding.ids ). The lines 3 and 5 display a similar pattern of code.</em> [Video description ends]<br><br>I'm going to access and print out to screen the Encoding_ids, the tokens, and the offsets.</p>
<p>Let's take a look at the tokenized text. The Encoding_ids are numeric representations of each word in the vocabulary. 5024 corresponds to 'We', 3662 corresponds to "'re", 3887 corresponds to 'check' with the space before it, and so on. The Encoding_tokens are, of course, the tokenized subwords for this text. The Encoding_offsets give the position of the subwords within the text. The first token 'We' has start index 0, end index is 2. The end index is not inclusive, it's exclusive. The second token apostrophe and re-starts at index position 2 and goes on till 5 and that's it.</p>
<p>We've built and trained our own BPE tokenizer using our small corpus of training data. There are additional tokenizer blocks that we haven't explored, for example, the post_processor. <br><br>[Video description begins] <em>Next cell reads: tokenizer.post_processor = processors.ByteLevel().</em> [Video description ends]<br><br>Now I'm going to use a ByteLevel post_processor first and I've instantiated that and added that to my tokenizer. I'll show you how this post_processor changes the output of the encoding. Let's encode the same sentence as before. "We're checking the pre-tokenization step". I call tokenizer.encode and I print out the Encoding_offsets. <br><br>[Video description begins] <em>Next cell displays various lines. Line 1 reads: sentence = "We're checking pre-tokenization step.". Line 3 reads: encoding = tokenizer.encode(sentence). Line 5 reads: print("Encoding_offsets :", encoding.offsets ).</em> [Video description ends]</p>
<p>Now the default ByteLevel post_processor does not include the space in front of the word as a part of the token. The offset for the token check is from 6 through 11. You can see the pair 6, 11 that correspond to the word check. Let me prove that to you. So, let's take a look at the start and end positions of the index position 2. <br><br>[Video description begins] <em>It reads: start, end = encoding.offsets[2].</em> [Video description ends]<br><br>That's the word check. Notice that the space is not part of the token. Let's take a look at the token at index position 9 and if you print out the start and end here, you can see that this corresponds to the token 'step'.</p>
<p>Again, the space is not included just before this token. Let's go back and change the configuration of the post_processor that we are using. We'll still use the ByteLevel post_processor, however, we'll set trim_offsets = False. <br><br>[Video description begins] <em>She scrolls upwards and edits a line of code. It now reads: tokenizer.post_processor = processors.ByteLevel(trim_offsets = False).</em> [Video description ends]<br><br>This indicates the post_processor that we should leave the offsets of tokens that begin with the special case character G, representing the space as they are. The offsets will point to the space before the word and not the first character of the word. Since the space is technically part of the token.</p>
<p>Let's rerun this code to look at the encoding offsets of the sentence. Look at the third offset, which was originally 6, 11. I haven't rerun this code cell yet. When I rerun it, you'll see that it changes to 5, 11. The offset pointing to the sub-token check now includes the space before it, and we can verify this by looking at the offsets for the word 'check'. Now when I rerun this, notice that it includes the space before the word. Let's do that for the token step as well. This time the sub-token includes the space before the token ' step'. You now know how the post-processor works. Now if you want to be able to decode a tokenized representation of your input text, you need to add in a decoder. <br><br>[Video description begins] <em>Line 1 reads: tokenizer.decoder = decoders.ByteLevel(). Line 3 reads: tokenizer.decode(encoding.ids).</em> [Video description ends]</p>
<p>For byte pair encoding, we use the ByteLevel decoder and I call tokenizer.decode on the IDs of the encoder and we get back the original sentence. Now let's say you wanted to use this tokenizer that you've trained with the GPT-2 model. You can wrap it in a PreTrainedTokenizerFast class as I've done here. A fast tokenizer is a Hugging Face tokenizer implemented in Rust which is much more performant than a tokenizer implemented in Python. This wrapped_tokenizer is something that we can use with the GPT-2 model, and you can see that it gives us the same result <br><br>[Video description begins] <em>Line 1 reads: from transformers import PreTrainedTokenizerFast. Line 3 reads: wrapped_tokenizer = PreTrainedTokenizerFast(. Line 4 reads: tokenizer_object = tokenizer,. Line 5 reads: bos_token = "&lt;|endoftext|&gt;",. Line 6 reads: eos_token = "&lt;|endoftext|&gt;",. Line 7 reads: ). Line 9 reads: wrapped_tokenizer.tokenize("We're checking pre-tokenization step.").</em> [Video description ends]<br><br>as the original tokenizer, it just wraps it. An alternative way to use your tokenizer with the GPT-2 model is to wrap it in the GPT2TokenizerFast object and this will give us the same result as before. <br><br>[Video description begins] <em>The following line is highlighted in the next cell: wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer).</em> [Video description ends]</p></div></div></a></div><div class="section"><a name="section_12"><h2 class="section_title">13. Video: Configuring the Normalizer and Pre-tokenizer for Wordpiece Tokenization (it_nlpllmdj_01_enus_13)</h2><img src="./Course Transcript_files/image001(12).jpg" alt="" width="300" onclick="(()=&gt;document.getElementById(&#39;lb-image&#39;).src = &#39;https://cdn2.percipio.com/public/b/db0359ae-bf55-4467-afe4-c9063f625f85/image001.jpg&#39;)();"><div class="section_text">In this video, you will learn how to perform normalization and pre-tokenization with WordPiece.</div><div class="section inner"><h3 class="list_title"></h3><ul class="list"><li class="list_item">perform normalization and pre-tokenization with WordPiece</li></ul><div class="section_text">[Video description begins] <em>Topic title: Configuring the Normalizer and Pre-tokenizer for Wordpiece Tokenization. Presented by: Janani Ravi.</em> [Video description ends]
<p>In this demo, once again, we'll build a tokenizer from scratch, block by block, using the different classes and libraries available from Hugging Face. In the previous demo, we built the byte pair encoding tokenizer. This time around, we'll build the WordPiece tokenizer. Having built up the functional blocks of the tokenizer, we'll train it on a small corpus of data and see what the tokenized output looks like for different sentences. The functional blocks to set up the tokenizer will be the same as in the BPE tokenizer, except that the WordPiece tokenizer uses a normalizer so that's an additional block that we'll configure.</p>
<p>Remember, the WordPiece tokenizer is the one that's used by the BERT models. Let's go ahead and get started. I'm going to pip install the transformers and datasets library, though they are already available in my Colab environment. <br><br>[Video description begins] <em>The TokenizersFromScratch_WordpieceTokenizer.ipynb page is open. The following code is highlighted: pip install transformers datasets.</em> [Video description ends]<br><br>But this is just to show you how you can do this or you can upgrade to a specific version if that's what you want to do. Once you have these libraries available in your runtime, let's first access the data that we are going to be using as the corpus to train our tokenizer, from datasets import load_dataset, and I call load_dataset and specify the name of the dataset ag_news. The split says, I only want the training data and I only want the first 15000 records. <br><br>[Video description begins] <em>The following lines are highlighted. Line 1 reads: from datasets import load_dataset. Line 4 reads: ag_news_dataset = load_dataset("ag_news", split = "train[:15000]").</em> [Video description ends]</p>
<p>This will load in the dataset object and you can see we have both text and a label and 15000 rows. Let's just get the text samples. On line 1, I run a little list comprehension to extract all of the text from the dataset so I'm left with only the text that we'll use to train our tokenizer. <br><br>[Video description begins] <em>Line 1 reads: text_samples = [example['text'] for example inag_news_dataset].</em> [Video description ends]<br><br>Next, I set up the imports for the different sub-modules that contain the different functional blocks of the tokenizer. You can see the imports on lines 1 through 9, <br><br>[Video description begins] <em>Line 1 reads: from tokenizers import (. Line 2 reads: decoders,. Line 3 reads: models,. Line 4 reads: normalizers,. Line 5 reads: pre_tokenizers,. Line 6 reads: processors,. Line 7 reads: trainers,. Line 8 reads: Tokenizer,. Line 9 reads: ).</em> [Video description ends] <br><br>I already explained all of the different sub-modules here. On line 11, I instantiate my WordPiece tokenizer, models.WordPiece gives me access to the WordPiece model. <br><br>[Video description begins] <em>Line 11 reads: tokenizer = Tokenizer(models.WordPiece(unk_token = "[UNK]")).</em> [Video description ends]</p>
<p>The WordPiece tokenizer for BERT does make use of the UNK token so I pass that in while instantiating the model. This will be the token that the model returns when it encounters characters or words that it hasn't seen before. Now that I have the tokenizer, the next step is to configure a normalizer that this tokenizer will use. We know normalization is a process, in that, we convert the input text to a consistent form. I've re-instantiated the BertNormalizer here. Since BERT is a widely used model. There is a built-in BertNormalizer class with all of the options that can be set for the BERT model.</p>
<p>The only option I have configured here is to set lowercase to False, so normalization will not lowercase the sentence. Now on line 3, I go ahead and I call tokenizer.normalizer.normalize_str, <br><br>[Video description begins] <em>Line 1 reads: tokenizer.normalizer = normalizers.BertNormalizer(lowercase = False).</em> [Video description ends]<br><br>and I pass in some text with accents. Now, because lowercase is set to False, you can see the normalized sentence. <br><br>[Video description begins] <em>Line 3 reads: tokenizer.normalizer.normalize_str("The pièce de rèsistance was the chef's Special Dessert.").</em> [Video description ends]<br><br>The characters are still in uppercase and the accents have not been stripped. Essentially, the normalizer has not done anything as far as we can tell. Let's configure the normalizer a little differently, the BertNormalizer, this time I'll set lowercase = True, and let's see how it performs the normalization now. And this time, not only is the result all in lowercase but accents have also been stripped.</p>
<p>So, it's pretty clear that in order to strip accents, you have to have lowercase turned on. Lowercase has to be set to True on the BertNormalizer. Let's set up our own normalizer rather than using the BertNormalizer. <br><br>[Video description begins] <em>Line 1 reads: tokenizer.normalizer = normalizers.Sequence(. Line 2 reads: [normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()].</em> [Video description ends]<br><br>You've seen this before, I used normalizers.Sequence to apply a bunch of normalization operations one after the other. The first is the NFD normalizer, then Lowercase, and then StripAccents. We've discussed the fact that NFD normalization is a kind of Unicode normalization where the base character and accents are separated into separate tokens and then StripAccents will get rid of the accents.</p>
<p>When you want to use StripAccents, you have to also use the NFD Unicode normalizer, otherwise, the StripAccents normalizer won't properly recognize the accented characters and thus won't strip them out. Now the sentence that I normalize has a bunch of spaces at the beginning, at the end of the sentence, and, of course, we have the usual accent characters in uppercase letters. <br><br>[Video description begins] <em>Line 5 reads: tokenizer.normalizer.normalize_str(" The pièce de rèsistance was the chef's Special Dessert. ").</em> [Video description ends]<br><br>Let's see what the output looks like. The text is all in lowercase, thanks to the lowercase normalizer And the accents have been removed, thanks to the combination of the NFD and StripAccents normalizer.</p>
<p>However, the spaces that you see at the beginning and the end of the sentence still exist. So, let's try and add one more normalizer to our sequence to strip out these spaces. On line 2, you can see the very first normalizer is normalizers.Strip. This will strip out any additional spaces at the beginning and end of the text. <br><br>[Video description begins] <em>The following line of code is highlighted in the next cell: [normalizers.Strip(), normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()].</em> [Video description ends] <br><br>And now when we try to normalize the same string, notice that the spaces have now been removed in the output result. The BERT model uses this additional normalization step. We can now move on to configuring the pre_tokenizer. We'll use the BertPreTokenizer available as a class in Hugging Face.</p>
<p>I've instantiated the pre_tokenizer on line 1, and we'll <br><br>[Video description begins] <em>Line 1 reads: tokenizer.pre_tokenizer = pre_tokenizer.BertPreTokenizer(). Line 3 reads: tokenizer.pre_tokenizer.pre_tokenize_str(" We're checking pre-tokenization step. ").</em> [Video description ends]<br><br>apply it on line 3, and let's see what the result looks like. The pre-tokenized output strips away all of the spaces. The spaces are not part of the tokens in the output, and every punctuation character is a different token. The BertPreTokenizer is what you'd use if you were planning to tokenize the data and feed into a BERT model but you can use other pre_tokenizers as well, such as the Whitespace tokenizer that I've instantiated here. <br><br>[Video description begins] <em>Line 1 reads: tokenizer.pre_tokenizer = pre_tokenizers.Whitespace().</em> [Video description ends] <br><br>Let's use the Whitespace tokenizer to tokenize the same sentence as before and this is what the resulting tokens look like. We tokenize on the space and the punctuation character. Observe that this tokenizes on special characters as well, pre and tokenization are separate tokens because of the hyphen between them, the dash.</p>
<p>But let's say, we'd written pretokenization without the hyphen as I've done here, pretokenization is a single word, this is what the tokenized result would look like, [Video description begins] <em>Line 1 reads: tokenizer.pre_tokenizer.pre_tokenize_str("We're checking the pretokenization step.").</em> [Video description ends] pretokenization would be a single token. This is how the Whitespace pre_tokenizer works. But let's say you were to use another pre_tokenizer, the WhitespaceSplit pre_tokenizer. It would work a little differently. <br><br>[Video description begins] <em>Line 1 reads: pre_tokenizer = pre_tokenizers.WhitespaceSplit().</em> [Video description ends]<br><br>This particular pre_tokenizer only splits on the Whitespace and not on the punctuation characters and other special characters. Observe that "We're" is a single token, as is pre-tokenization and the period is part of the token step.</p>
<p>The Whitespace pre_tokenizer is a combination of the WhitespaceSplit pre_tokenizer and the punctuation pre_tokenizer. I've set these up as a pre_tokenizers.Sequence you can see <br><br>[Video description begins] <em>Line 1 reads: pre_tokenizer = pre_tokenizers.Sequence(. Line 2 reads: [pre_tokenizers.WhitespaceSplit(), pre_tokenizers.Punctuation()].</em> [Video description ends]<br><br>on lines 1 through 3. We'll use this pre_tokenizers.Sequence to pre_tokenize this string that you see here on screen and this is what the result look like. This is the same result that we got from the Whitespace pre_tokenizer. We split on the Whitespace, and other special characters, and punctuation.</p></div></div></a></div><div class="section"><a name="section_13"><h2 class="section_title">14. Video: Building and Training a Wordpiece Tokenizer (it_nlpllmdj_01_enus_14)</h2><img src="./Course Transcript_files/image001(13).jpg" alt="" width="300" onclick="(()=&gt;document.getElementById(&#39;lb-image&#39;).src = &#39;https://cdn2.percipio.com/public/b/f36704b2-b5df-4161-908a-729bf564e76a/image001.jpg&#39;)();"><div class="section_text">Discover how to train a WordPiece tokenizer.</div><div class="section inner"><h3 class="list_title"></h3><ul class="list"><li class="list_item">train a WordPiece tokenizer</li></ul><div class="section_text">[Video description begins] <em>Topic title: Building and Training a Wordpiece Tokenizer. Presented by: Janani Ravi.</em> [Video description ends]
<p>We are now ready to train our WordPiece tokenizer on the training data that we have, and for that, we need to instantiate a trainer. We set up the special_tokens that our tokenizer needs to recognize the UNK, PAD, SEP, MASK, and CLS tokens. We instantiate the WordPieceTrainer on line 3. <br><br>[Video description begins] <em>The TokenizersFromScratch_WordpieceTokenizer.ipynb page is open. She highlights the following line of code: special_tokens = ["[UNK]", "[PAD]", "[CLS]", "[SEP]", "[MASK]"].</em> [Video description ends]<br><br>vocab_size is 20000, special_tokens set to the special_tokens, we've set up, and min_frequency = 2. <br><br>[Video description begins] <em>Line 3 reads: trainer = trainers.WordPieceTrainer(vocab_size = 20000, special_tokens = special_tokens, min_frequency = 2). Line 5 reads: trainer.</em> [Video description ends] <br><br>We'll only consider tokens to be part of the vocabulary if they occur at least two times in our corpus. Sub-words that occur less frequently than this threshold will be excluded from the vocabulary. Once we instantiated the trainer, let's start the training process.</p>
<p>I call tokenizer.train_from_iterator, pass in the text_samples, and the WordPiece trainer and this will kick start the training process. <br><br>[Video description begins] <em>Next cell reads: tokenizer.train_from_iterator(text_samples, trainer = trainer).</em> [Video description ends]<br><br>Now that we have a trained tokenizer, we can test the tokenizer by calling tokenizer.encode and passing in the input text that we want tokenized, we get an Encoding object.<br><br>[Video description begins] <em>It reads: encoding = tokenizer.encode("We're checking pre-tokenization step.").</em> [Video description ends]<br><br>You can see num_tokens=11, and there are a number of different attributes as well. This includes the ids, the offsets, the attention_mask, and so on and so forth. I won't discuss all of these attributes. Instead, we'll only look at the Encoding_ids, tokens, and offsets.</p>
<p>If you look at the results printed out to screen, you can <br><br>[Video description begins] <em>Line 1 reads: print("Encoding_ids :", encoding.ids ). The lines 3 and 5 display a similar pattern of code.</em> [Video description ends]<br><br>see that the Encoding_ids are integer representations of the tokens in our vocabulary. The integer 448 corresponds to the first sub-word token we, the integer 10 corresponds to the single apostrophe, 140 corresponds to re, and so on and so forth. Offsets in the encoding give us the offsets of every token in the input text, the start index and the end index. The end index is not inclusive. The last step in the tokenization pipeline is post-processing. Now, before we set up a post-processor, we need to get the numeric IDs representing some of the special tokens that we have, specifically the CLS and SEP token. <br><br>[Video description begins] <em>Line 1 reads: cls_token_id = tokenizer.token_to_id("[CLS]"). Line 2 displays a similar pattern of code.</em> [Video description ends]</p>
<p>CLS stands for classification and its a special_token in the BERT transformer model added to the beginning of the input text. Now, the reason I'm printing out these special token IDs is because we need to be able to identify them in our tokenized sentence. I'll show you how you can set up a post-processor for our tokenized output to match the format that the BERT model expects. Now, the BERT model expects a CLS token at the beginning of the sentence and a SEP token at the end of the sentence, or if you're passing in a pair of sentences, the SEP will be at the end of each sentence. Now, we are not working with sentence pairs, only with single sentences. So, we need to identify this CLS and SEP tokens, and you can see they have IDs 2 and 3.</p>
<p>So, when we print out the Encoding_ids, we'll be able to identify the CLS and SEP token. Now how do we get our tokenized representation in the format that the BERT model expects? We'll use the TemplateProcessing class to achieve this. This will be our post_processor. <br><br>[Video description begins] <em>Line 1 reads: tokenizer.post_processor = processors.TemplateProcessing(.</em> [Video description ends]<br><br>The TemplateProcessing class uses the template that we specify to structure the output of the tokenizer, and this output is what will be fed into our model for training. Observe that I've specified the template for a single sentence on line 2, a pair of sentences on line 3, and I've also <br><br>[Video description begins] <em>Line 2 reads: single = f"[CLS]:0 $A:0 [SEP] :0",. Line 3 reads: pair = f"[CLS]:0 $A:0 [SEP] :0 $B:1 [SEP] :1 ",.</em> [Video description ends]<br><br>indicated what special_tokens I've used, the CLS token <br><br>[Video description begins] <em>Line 4 reads: special_tokens = [("[CLS]", cls_token_id), ("[SEP]", sep_token_id )],.</em> [Video description ends]<br><br>corresponding to the cls_token_id and the SEP token corresponding to the sep_token_id.</p>
<p>Now let's take a look at the template that I've set up here. The single template is used when the input to the tokenizer is a single sentence. Now, the template specification that I have here indicates that at the start of the sentence, we should insert a CLS token, and at the end of the sentence, we should insert a SEP token. The $A on line 2 represents the tokenized input sequence and observe that after the CLS token $A, and the SEP token, we have a :0. This :0 after each token indicate that all of these tokens belong to the first and only sequence segment. Now the template specified as the pair input argument on line 3 is the template used when the input consists of a pair of sequences, that is 2 sentences.</p>
<p>The BERT model requires this pair representation for question-answering tasks when there are two input sentences passed into the model. Let's look at this template. Notice we have the CLS token at the very beginning. This is followed by the $A which represents the first input sequence. Then we have the SEP token and then $B represents the second sentence or the second input sequence. The :0 after the CLS token, $A, and the first SEP token indicates that this segment belongs to the first sequence or the first sentence. The :1 after $B and the second SEP indicates that this segment belongs to the second sequence. This is the template that will be used to post-process the output of the tokenizer to feed into BERT models.</p>
<p>Now that we've added a post_processor to our tokenizer as well, let's go ahead and see how we can tokenize the input text. I call tokenizer.encode. Print out the IDs, tokens, offsets, and the type_ids. <br><br>[Video description begins] <em>Line 1 reads: encoding = tokenizer.encode("We're checking pre-tokenization step."). Line 3 reads: print("Encoding_ids:", encoding.ids). Line 4 reads: print(). Line 5 to Line 9 display a similar pattern of codes.</em> [Video description ends] <br><br>We've encoded just a single sentence here. Observe that the Encoding_ids starts with 2, that is the CLS token, and ends with 3, that is the SEP token. You can see the corresponding tokens in the Encoding_tokens output. The offsets are the same as before. Notice the Encoding_type_ids. Those identify the sequence segment for the encoded output and all 0s here imply that this is the first and only sequence segment.</p>
<p>We also have the template configured for a pair of sentences. So, let's try and encode a pair of sentences using our tokenizer. <br><br>[Video description begins] <em>Line 1 reads: encoding = tokenizer.encode("We're checking pre-tokenization step.", "on a pair of sentences."). Line 3 reads: print("Encoding_ids:", encoding.ids). Line 4 reads: print(). Line 5 to Line 9 display a similar pattern of codes.</em> [Video description ends]<br><br>So, We're checking the pre-tokenization step, that's the first sequence, on a pair of sentences, that's the second sequence. We'll print out the Encoding_ids, tokens, offsets, and type_ids as before and this is what the result looks like. In the Encoding_ids, the first token has integer value 2 corresponding to the CLS token. You can see the Encoding_id 3 somewhere in the middle that corresponds to the SEP token separating the 2 sentences. And let me scroll over to the right, you can see the Encoding_id 3 that corresponds to the SEP token at the end of the second sentence.</p>
<p>Another interesting thing to note are the type_ids. Notice that the Encoding_type_ids is set to 0 for the first input sequence and set to 1 for the second input sequence. This is how the separate input sequences or sentences are identified by the model. If you want to be able to decode the tokenized output and get back the original sentence, you need to configure a decoder for the tokenizer. <br><br>[Video description begins] <em>Line 1 reads: tokenizer.decoder = decoders.WordPiece(prefix = "##"). Line 3 reads: tokenizer.decode(encoding.ids).</em> [Video description ends]<br><br>I've used the WordPiece decoder here and notice I specify the prefix, the ##. This is the prefix that the tokenizer added to the sub-word tokens that were not the first token in a word to indicate the continuation of a previously started word.</p>
<p>That's called tokenizer.decode on the encoding.ids and here is the result. 'we ' re checking pre - tokenization step. on a pair of sentences.' This is the reconstruction of the original sentence. Once you have a trained tokenizer, you can save it out in the JSON format, <br><br>[Video description begins] <em>Next cell reads: tokenizer.save("word_piece_tokenizer.json").</em> [Video description ends]<br><br>tokenizer.save, then the name of a JSON file will save out the tokenizer. And if you have a JSON file representing a tokenizer, you can always load your tokenizer from this file. <br><br>[Video description begins] <em>Line 1 reads: new_tokenizer = Tokenizer.from_file("word_piece_tokenizer.json").</em> [Video description ends] <br><br>Tokenizer.from_file will load and initialize your tokenizer. Let's use this new tokenizer to perform the same tokenization as before. "We're checking the pre-tokenization step.", "on a pair of sentences." And you can see that the output is identical. You can see the Encoding_tokens are identical to what we got before.</p>
<p>This is the same tokenizer which we just loaded from a JSON file. Now you've trained your own tokenizer and you want to use it with a Hugging Face transformer model, you need to wrap it in a fast tokenizer.<br><br>[Video description begins] <em>She highlights the following lines of code. Line 3 reads: wrapped_tokenizer = PreTrainedTokenizerFast(. Line 4 reads: tokenizer_object = tokenizer,. Line 5 reads: unk_token = "[UNK]",. Line 6 reads: pad_token = "[PAD]",. Line 7 reads: cls_token = "[CLS]",. Line 8 reads: sep_token = "[SEP]",. Line 9 reads: mask_token = "[MASK]",. Line 10 reads: ).</em> [Video description ends] <br><br>Here on lines 3 through 10, I've wrapped my original tokenizer using PreTrainedTokenizerFast and I've specified all of the special tokens as well. Now using this wrapped_tokenizer, I'm going to tokenize the pair of sentences that we had earlier. And here is what the tokenized representation looks like. Now, it's not just the pre-trained wrapped_tokenizer, you can use the BertTokenizerFast as well to wrap your tokenizer. <br><br>[Video description begins] <em>She highlights a line. It reads: wrapped_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer).</em> [Video description ends]<br><br>Here I don't specify the special_tokens because the BertTokenizerFast is already aware of these special tokens used by the BERT model. And here is the tokenized representation of our text.</p></div></div></a></div><div class="section"><a name="section_14"><h2 class="section_title">15. Video: Course Summary (it_nlpllmdj_01_enus_15)</h2><img src="./Course Transcript_files/image001(14).jpg" alt="" width="300" onclick="(()=&gt;document.getElementById(&#39;lb-image&#39;).src = &#39;https://cdn2.percipio.com/public/b/a1d515e7-1653-4d4b-a69e-e87cbdb5b919/image001.jpg&#39;)();"><div class="section_text">In this video, we will summarize the key concepts covered in this course.</div><div class="section inner"><h3 class="list_title"></h3><ul class="list"><li class="list_item">summarize the key concepts covered in this course</li></ul><div class="section_text">[Video description begins] <em>Topic title: Course Summary. Presented by: Janani Ravi.</em> [Video description ends]
<p>We've now reached the end of this course on working with tokenizers in Hugging Face. We started this course off by introducing and exploring the Hugging Face platform. This is a machine learning platform and community that enables users to build, train, fine-tune, and deploy all kinds of deep learning models, especially those related to natural language processing. Hugging Face offers numerous libraries for working with text data such as the transformers library, which offers a wide variety of pre-trained transformer models for different NLP tasks.</p>
<p>Through Hugging Face, we gain access to a plethora of pre-trained models and datasets along with the comprehensive toolkit for NLP tasks. We explored the Hugging Face platform in depth, discovering how its tools and libraries such as transformers can be leveraged to build, train, and deploy state-of-the-art NLP models efficiently. Next, we learned about the technical setup required to harness the power of these models, we set up the Google Colab environment. This cloud-based platform enabled us to write and execute Python code through the browser, providing a seamless and accessible way to experiment with complex NLP models without requiring powerful hardware.</p>
<p>Additionally, we explore the intricacies of normalizers and pre-tokenizers, essential component in the text pre-processing pipeline. These are two steps usually performed on text before the actual tokenization, that is, before the splitting of sequences into individual words or subwords. Normalization involved removing special characters and converting text to lowercase. Pre-tokenization, as the name suggests, involved converting text to preliminary tokens, typically by splitting on the whitespace or using some other simple algorithm.</p>
<p>These tools helped us in refining text data, ensuring it's in the optimal format for tokenization and subsequent processing by AI models, thus improving the efficiency and accuracy of NLP tasks. Finally, we explored various tokenization methods that are fundamental to understanding and utilizing transformer models effectively. We learned that tokenizers used in transformers performed subword tokenization to be able to have a vocabulary of a manageable size. We took an in-depth look at byte pair encoding, WordPiece, and Unigram tokenization techniques.</p>
<p>Through practical exercises, we implemented BPE tokenization, understanding its mechanism for reducing vocabulary size and handling unknown words. We also built and trained a BPE tokenizer, followed by similar exercises for WordPiece tokenization. In conclusion, this course has provided us with a solid understanding of the basics of Hugging Face and text tokenization, enabling us to move on to Hugging Face pipelines for NER, question-answering, and text generation in the course coming up ahead.</p></div></div></a></div><div class="section"><a name="section_15"><h2 class="section_title">Course File-based Resources</h2><table class="grid"><tbody><tr class="gridrow"><td class="grid_col">•</td><td class="grid_col"><a class="section_link" href="https://cdn2.percipio.com/secure/c/9999999999.0aa9780ac187ce43e11abd8f17edd315ec9272b3/eot/resources/saved/8b9b662e-c13a-4988-886f-1bbbddfef3d1/it_nlpllmdj_01_assets.zip">NLP with LLMs: Working with Tokenizers in Hugging Face</a></td></tr><tr class="gridrow"><td class="grid_col"></td><td class="grid_col">Topic Asset</td></tr></tbody></table></a></div><div class="copyright-container"><a name="section_15"><span class="copyright-text">© 2024 Skillsoft Ireland Limited - All rights reserved.</span></a></div></div><div id="lb-overlay"><div class="lb-header"><a name="section_15"><button onclick="(()=&gt;{const overlay = document.getElementById(&#39;lb-overlay&#39;);overlay.style.visibility = &#39;hidden&#39;;const curtain = document.getElementById(&#39;lb-curtain&#39;);curtain.style.opacity = 1.0;curtain.style.backgroundColor = &#39;white&#39;;curtain.style.pointerEvents = &#39;auto&#39;;})();">X</button></a></div><div id="lb-image-container"><a name="section_15"><img id="lb-image" alt="" onload="(()=&gt;{const curtain = document.getElementById(&#39;lb-curtain&#39;);curtain.style.opacity = 0.5;curtain.style.backgroundColor = &#39;grey&#39;;curtain.style.pointerEvents = &#39;none&#39;;const overlay = document.getElementById(&#39;lb-overlay&#39;);overlay.style.visibility = &#39;visible&#39;;window.scrollTo(0, 0);})();"></a></div><div class="lb-footer"></div></div></div></body></html>